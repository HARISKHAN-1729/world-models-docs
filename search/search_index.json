{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"The Epistemological Crisis of Generative AI The field of artificial intelligence currently finds itself at a critical turning point. The dominant approach, driven by the enormous success of Large Language Models (LLMs) and systems that generate content automatically, has produced results that are impressively fluent. These systems can use language with a skill that appears similar to human reasoning, yet they remain fundamentally disconnected from the physical reality they claim to describe. This report argues that the current path\u2014making auto-regressive models larger and larger with more parameters and more data\u2014is approaching a limit that falls short of true Autonomous Machine Intelligence (AMI) [LeC22] . The main problem with the generative approach lies in how it learns: by predicting the next token (word piece) or reconstructing the next pixel. This is a mechanism of approximation, not genuine understanding. When an LLM predicts how a physical object will move, it does not actually simulate the physics of mass and momentum. Instead, it retrieves a statistical pattern of how humans typically describe such movements in text. It works in the separate, symbolic world of language, or the complex, noisy world of pixels, rather than in the continuous, abstract space of underlying reality [Sha22] . To close the gap between statistical imitation and real understanding, we must move away from the goal of generation and toward prediction in latent space. We must build systems that construct an internal World Model\u2014a simulation of the environment that captures meaningful cause and effect while filtering out the irrelevant details of sensory noise. This document serves as both a foundational text and a practical guide for constructing such a system. It explains the theoretical shift from probabilistic models to Energy-Based Models (EBMs), details the Joint-Embedding Predictive Architecture (JEPA), and provides a complete, step-by-step methodology for implementing a Video-JEPA (V-JEPA) using the Moving MNIST dataset as a simplified example of physical reality. The Auto-Regressive Trap: System 1 without System 2 To understand why we need World Models, we must first identify the problems with current AI systems. Auto-regressive models, such as the Transformer architectures that power GPT-4 or Llama, work in a way similar to \u201dSystem 1\u201d thinking in human psychology\u2014fast, automatic, and reactive [K\u013125] . They generate responses one token (word piece) at a time, with each step based only on what came before. This process is fundamentally unable to plan ahead. Planning requires the ability to imagine multiple possible futures, evaluate how good each outcome would be, and choose the best path before taking the first action. An auto-regressive model, by its very nature, commits to an output immediately. It speaks without thinking first [Let24] . Additionally, applying this generative approach to visual information\u2014predicting the next video frame pixel by pixel\u2014is both computationally expensive and conceptually flawed. The world is complex and filled with random, unpredictable details. The texture of a carpet, the random movement of leaves in the wind, or the exact reflection of light on water are high-entropy details that are largely unpredictable and often irrelevant to the actual task. A generative model trained to reconstruct these pixels must use enormous capacity to model this noise. If it fails to predict the exact texture of a leaf, it receives a high error penalty, even if it correctly predicts where the leaf is generally located. This misalignment of goals forces the model to focus on small, detailed features at the expense of understanding high-level, meaningful dynamics.Claude is AI and can make mistakes. Please double-check responses [Sha22] . The Biological Counter-Proof Biological intelligence provides clear evidence that pixel-level prediction is unnecessary for understanding. A human infant does not learn the physics of the world by predicting the brightness value of every light receptor in their retina. Instead, the infant observes the world and builds an internal abstraction\u2014a model\u2014of object permanence, gravity, occlusion, and inertia. By the age of a few months, long before learning language, an infant possesses \u201dcommon sense\u201d physics. They show surprise when an object passes through a solid wall or when it disappears without explanation. This surprise is the result of a prediction error in their internal World Model. This World Model allows biological agents to perform \u201dSystem 2\u201d thinking: slow, careful reasoning and planning. It enables an agent to imagine the consequences of its actions without risking physical harm. It allows for the simulation of \u201dwhat if\u201d scenarios (\u201dWhat would happen if I dropped this glass?\u201d). The goal of AMI research is to create this capability in artificial systems. We must move from machines that reproduce the surface appearance of the world to machines that understand the underlying structure of the world. The Architecture of Autonomous Machine Intelligence To replicate the capabilities of biological intelligence, a modular cognitive architecture for AMI was proposed in [LeC22] . This architecture is not a single neural network but a system of interacting components, centered around the World Model. Each module plays a distinct role in the perception-action loop, enabling the agent to reason, plan, and learn from observation (Figure 1). Figure 1: System architecture for autonomous intelligence with differentiable modules: configurator, perception, world model, cost (intrinsic + critic), short-term memory, and actor. The Six Core Modules The architecture comprises six distinct modules, each differentiable and trainable, allowing gradients to propagate through the entire system (Table 1). \ud83e\udde9 Module \u2699\ufe0f Function \ud83e\uddec Biological Analogy Configurator Acts as the executive controller. Sets goals and dynamically modulates the parameters of other modules based on the current task. Determines what the agent should do . Prefrontal Cortex (Executive Function) Perception Estimates the current world state s\u209c from sensory input x\u209c . Filters noise and extracts relevant semantic features. Sensory Cortex (Visual / Auditory) World Model Internal simulator. Predicts future states s\u209c\u208a\u2081 from current states s\u209c and hypothetical actions a\u209c . Can infer missing state information. Hippocampus / Frontal Cortex Cost Computes the \u201cenergy\u201d or \u201cdiscomfort\u201d of a state. Combines intrinsic drives (e.g., energy minimization) with extrinsic, task-specific objectives. Amygdala / Basal Ganglia (Reward\u2013Pain System) Actor Proposes candidate action sequences to minimize predicted future cost. Does not act directly\u2014feeds proposals to the World Model for evaluation. Premotor Cortex Short-Term Memory Maintains recent sequences of states, actions, and costs, enabling temporal context for prediction and planning. Working Memory The Centrality of the World Model Among these, the World Model is the most critical and the most challenging to construct. It is the engine of prediction. The World Model must satisfy two competing requirements: Informativeness: The state representation must contain enough information to distinguish between functionally different situations (e.g., the difference between a car moving towards you vs. away from you). Predictability: The state representation must discard information that is unpredictable or irrelevant (e.g., the exact pattern of clouds in the sky, unless the task is weather forecasting). Standard generative models fail the second requirement by trying to predict everything. Purely invariance- based models (like contrastive learning) risk failing the first by collapsing distinct states into identical rep- resentations if the data augmentation views are too aggressive [Und25] . The Joint-Embedding Predictive Architecture (JEPA) is designed specifically to navigate this trade-off. Theoretical Foundations: From Probability to Energy The mathematical framework underpinning most current AI is probabilistic modeling. We attempt to estimate \\(P(Y|X)\\) , the probability distribution of the output given the input. The Normalization Constraint While rigorous, this framework imposes a severe constraint: the distribution must normalize (sum to one). In high-dimensional spaces like video, calculating the normalization constant (the partition function ) is intractable. This forces researchers to: Rely on approximations. Restrict the model to simplified distributions (like Gaussians in VAEs) that do not match the complexity of the real world [LeC]. Energy-Based Models (EBMs) We advocate for abandoning the probabilistic straitjacket in favor of Energy-Based Models (EBMs) . An EBM does not attempt to model probabilities. Instead, it defines a scalar energy function \\(E(X, Y)\\) that measures the \"compatibility\" between an input \\(X\\) and a potential output \\(Y\\) . Low Energy (Compatible) The pair \\((X, Y)\\) is compatible. \\(Y\\) is a plausible continuation of \\(X\\) . High Energy (Incompatible) The pair \\((X, Y)\\) is incompatible. \\(Y\\) is physically impossible or semantically unrelated to \\(X\\) . Inference in an EBM becomes an optimization problem: finding the \\(Y\\) that minimizes the energy for a given \\(X\\) : \\[ Y^* = \\underset{Y}{\\mathrm{argmin}} \\, E(X, Y) \\] This framework removes the need for normalization, granting us immense flexibility in designing the architecture of the energy function. The Collapse Problem in Self-Supervised Learning The primary challenge in training EBMs, particularly in self-supervised learning (SSL) where we do not have labels, is Collapse . In SSL, we typically have pairs of compatible data points \\((x, y)\\) \u2014for example, two frames from the same video. We want to train the model such that \\(E(x, y)\\) is low. The Naive Approach A naive approach would simply minimize the distance between their representations: \\[ L = \\| \\text{Enc}(x) - \\text{Enc}(y) \\|^2 \\tag{2} \\] The Catastrophic Failure Mode The fatal flaw of this objective is that the encoder learns to map every input to a constant vector (e.g., zero). \\[ \\text{Enc}(x) = 0, \\quad \\text{Enc}(y) = 0 \\implies L = 0 \\tag{3} \\] The loss is minimized perfectly, but the representations contain zero information . The energy surface becomes completely flat (zero everywhere), making the model useless for discrimination or planning. Strategies for Preventing Collapse To train a useful EBM, we must ensure that the energy is low for compatible pairs (positive samples) and high for incompatible pairs (negative samples). We term this \"shaping the energy landscape.\" There are three primary families of techniques to achieve this [Red25b] : 1. Contrastive Methods Explicitly push up the energy of negative samples. This requires mining \"negative\" pairs\u2014inputs that are definitely unrelated. Pros: Highly effective (e.g., SimCLR). Cons: Inefficient because the space of \"incorrect\" answers is infinite. The model must see a vast number of negatives to define the energy boundary [Mina] . 2. Regularization Methods Impose constraints on the information content of the embeddings. Methods like VICReg (Variance-Invariance-Covariance Regularization) explicitly penalize collapse by adding loss terms: Variance Term: Forces neurons to vary across the batch (prevents constant output). Covariance Term: Forces neurons to capture different features (prevents redundancy). 3. Architectural Methods (The JEPA Approach) Design the architecture such that the target representation is not fixed but evolves in a way that the predictor cannot easily cheat. This often involves asymmetry , such as using a \"Teacher\" network that is an Exponential Moving Average (EMA) of the \"Student\" network. This creates a moving target that prevents the student from converging to a trivial constant solution [Red25a] . Joint-Embedding Predictive Architectures (JEPA) The JEPA represents the convergence of World Modeling theory and Energy-Based collapse prevention. Non-Generative by Design JEPA is a non-generative architecture : it does not reconstruct \\(x\\) . Instead, it predicts the latent representation of \\(y\\) from the latent representation of \\(x\\) . The Core Mechanism The JEPA consists of three primary sub-networks: Context Encoder (Student): Processes the observed part of the input (the context, \\(x\\) ) to produce a representation \\(s_x\\) . Target Encoder (Teacher): Processes the part of the input to be predicted (the target, \\(y\\) ) to produce a representation \\(s_y\\) . Predictor: A network that takes \\(s_x\\) (and potentially a latent variable \\(z\\) or action \\(a\\) ) and outputs a prediction \\(\\hat{s}_y\\) . The objective is to minimize the distance between the prediction and the target in embedding space: \\[ L = D(\\hat{s}_y, s_y) \\tag{4} \\] Preventing Collapse via EMA Critically, the Target Encoder is not updated via gradient descent from this loss. Doing so would allow the two encoders to conspire to output a constant. Instead, the Target Encoder's weights \\(\\phi\\) are updated as an Exponential Moving Average (EMA) of the Context Encoder's weights \\(\\theta\\) : \\[ \\phi_t \\leftarrow \\tau \\phi_{t-1} + (1 - \\tau)\\theta_t \\tag{5} \\] Where \\(\\tau\\) is a decay parameter typically close to 1 (e.g., 0.996). This asymmetry ensures that the target representation is stable and semantically rich, forcing the predictor to learn the underlying dynamics to match it. I-JEPA: Image-Based World Modeling I-JEPA applies this principle to static images. It treats the image as a \"world\" where spatial relationships define the physics. Context: A subset of image patches. Target: A different, masked subset of patches. By predicting the embeddings of the missing patches, the model learns high-level semantic features (object parts, shapes) without ever generating pixels. Overcoming Texture Bias This approach avoids the \"texture bias\" of generative models like Masked Autoencoders (MAE) . While MAE wastes capacity reconstructing high-frequency noise (pixels), I-JEPA focuses solely on semantic content by operating in the latent space. V-JEPA: The Engine of Physical Understanding V-JEPA extends this to the temporal domain, which is the natural domain of World Models. In V-JEPA, the input is a video sequence. Context: A set of spatio-temporal blocks (tubelets) from the video. Target: A different set of tubelets, often representing the future frames or occluded regions. Physics as Prediction: By predicting the representation of future frames, the model implicitly learns the laws of physics. To predict where a ball will be in \\(t + 1\\) , it must understand velocity and inertia. To predict the representation of a person walking behind a tree, it must understand occlusion and object permanence [App25] . Crucial: The Masking Strategy The masking strategy in V-JEPA is critical to success. Random Masking (Bad): If we mask random scattered tubelets, the model can simply interpolate from neighbors (spatial smoothing). Block Masking (Good): To force the learning of dynamics, we must mask entire blocks of time or space . This forces the model to bridge the gap using its understanding of motion rather than local texture statistics [ABB+23]. The Challenge of Stochasticity and Latent Variables The real world is not deterministic . Deterministic: If an agent drops a pen, it will fall. Stochastic: If an agent observes a car approaching an intersection, the car might turn left, right, or go straight. The Flaw of Averaging A World Model that outputs a single deterministic prediction \\(\\hat{s}_{t+1}\\) will inevitably predict the average of all possible futures. In embedding space, the average of a \"left-turn representation\" and a \"right-turn representation\" might be a \"crash-into-the-divider representation\" or a blurry, non-descript state. Latent Variable \\(z\\) Handling Multimodal Futures To handle multimodal futures, the JEPA architecture supports a latent variable \\(z\\) . The predictor becomes a function of both the context and this latent variable: \\[ \\hat{s}_y = \\text{Predictor}(s_x, z) \\tag{6} \\] The variable \\(z\\) encodes the information present in the future ( \\(y\\) ) that is not present in the past ( \\(x\\) ). Generative vs. EBM Frameworks The treatment of \\(z\\) differs fundamentally between paradigms: Generative Framework: We would sample \\(z\\) from a fixed prior distribution. EBM Framework: \\(z\\) is an input that minimizes the energy . \\[ E(x, y) = \\min_z D(\\hat{s}_y(z), s_y) \\tag{7} \\] This allows the model to \"explain away\" the uncertainty. For a specific future \\(y\\) (e.g., the car turned left), there exists a \\(z\\) (representing the \"decision to turn left\") that makes the prediction match the target. Determinism in Simplified Environments Practical Implementation: Moving MNIST For the practical implementation guide in this report, we will focus on the Moving MNIST dataset. This environment is largely deterministic: Dynamics: Digits move with constant velocity. Physics: Digits reflect off walls deterministically. Ambiguity: There is only minor ambiguity during occlusion (e.g., determining which digit is on top). Simplification Strategy Because the physics are rigid, we can initially simplify our architecture by omitting the explicit latent \\(z\\) . We rely on the inherent capacity of the predictor to approximate the dominant mode of the distribution. This reduces architectural complexity for the learner while still capturing the core principles of the JEPA. Practical Implementation Strategy: The Micro-World We now transition from theory to practice. The request requires a step-by-step guide to building a World Model using a small, open-source dataset. The Selection We select Moving MNIST as the ideal \"Micro-World.\" Why Moving MNIST? Moving MNIST consists of sequences (typically 20 frames) of \\(64 \\times 64\\) pixel images containing two handwritten digits bouncing inside a frame. Physics: It exhibits strict physical laws: conservation of momentum (velocity), reflection (bouncing), and depth (occlusion). Manifold Hypothesis: The high-dimensional pixel space ( \\(20 \\times 64 \\times 64 = 81,920\\) dimensions) is generated by a very low-dimensional set of latent variables: the position \\((x, y)\\) , velocity \\((v_x, v_y)\\) , and digit identity ( \\(ID\\) ) for two objects. A successful World Model should theoretically compress the video down to this low-dimensional manifold. Compute Efficiency: Unlike Kinetics-400 or real-world robotics data, Moving MNIST can be trained on a single consumer GPU (or even a high-end CPU in a reasonable time for debugging), making it accessible for a \"from scratch\" guide. The Data Structure The dataset is typically generated on-the-fly to prevent overfitting (infinite data regime) or downloaded as a fixed set. Input Tensor: (Batch, Time, Height, Width) Dimensions: (B, 16, 64, 64) (We use 16 frames for standard V-JEPA input). Normalization: Pixels should be normalized to the range [0, 1] or [-1, 1] . Self-Supervised Learning We do not use the class labels (which digit is which) for training. This is Self-Supervised Learning . The signal comes from predicting the masked portions of the video, not from ground-truth labels. Phase 1 Phase 1 is the foundation. If the data doesn't obey physics, the World Model has nothing to learn. The Architect We are acting as the \"Creator\" of this Micro-World. We define the space, time, matter, and the laws of physics. Overview: The 5-Step Recipe We will break the data generation into five logical steps: The Universe: Defining the empty void (Space and Time). The Atoms: Getting the raw matter (MNIST digits). The Big Bang: Setting initial positions and velocities. The Laws of Physics: Updating positions and handling bounces. Rendering: Drawing the \"atoms\" onto the \"universe\" (handling overlap). Step 1: The Universe (The 5D Tensor) In Deep Learning, a video is just a 5-dimensional block of numbers. The Dimensions: (Batch, Time, Channels, Height, Width) Batch (B): How many parallel universes we simulate at once (e.g., 16 videos). Time (T): The length of history (e.g., 16 frames). Channels (C): 1 (Black & White). Height/Width (H, W): \\(64 \\times 64\\) pixels. void.py 1 2 3 4 5 6 7 8 # --------------------------------------------------------- # INITIALIZE THE VOID # --------------------------------------------------------- # Create an empty universe filled with zeros (black) # Target Shape: [Batch, Time, Channels, Height, Width] # Dimensions: [16, 16, 1, 64, 64] videos = torch . zeros ( batch_size , seq_len , 1 , 64 , 64 ) Step 2: The Atoms (The Digits) We need objects. We use standard MNIST handwritten digits. The Problem: Standard MNIST is \\(28 \\times 28\\) pixels. Our world is \\(64 \\times 64\\) . The Preparation: We load the MNIST dataset and keep it in memory. We treat these \\(28 \\times 28\\) grids as our \"solid objects.\" atoms.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # --------------------------------------------------------- # HARVESTING MATTER # --------------------------------------------------------- # We need raw materials to populate the universe. # Source: Standard MNIST handwritten digits. from torchvision.datasets import MNIST # 1. Download and Load MNIST # Root: Where the raw data is stored locally mnist = MNIST ( root = './data' , train = True , download = True ) # 2. Normalize and Float # Convert integer pixels [0, 255] to float [0.0, 1.0] data = mnist . data . float () / 255.0 print ( f \"Matter Harvested: { data . shape } \" ) # Output: torch.Size([60000, 28, 28]) Step 3: The Big Bang (Initialization) For every video in the batch, we need to decide where the digits start and where they are going. Position ( \\(P\\) ): A random coordinate \\((x, y)\\) inside the \\(64 \\times 64\\) box. Constraint: We don't want the digit to spawn strictly on the edge, so we limit the spawn area to \\(64 - 28 = 36\\) . Velocity ( \\(V\\) ): How fast and in what direction? We pick a random angle \\(\\theta\\) between \\(0\\) and \\(2\\pi\\) ( \\(360^\\circ\\) ). We convert this angle into a velocity vector \\((v_x, v_y)\\) using trigonometry: \\[ v_x = \\cos(\\theta) \\tag{8} \\] \\[ v_y = \\sin(\\theta) \\tag{9} \\] big_bang.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # --------------------------------------------------------- # INITIALIZE TRAJECTORIES # --------------------------------------------------------- # Determine initial state vectors for all particles (digits). import math # 1. Random Position (x, y) # Limit spawn range to [0, 36] to prevent clipping at t=0 # (Universe Width 64) - (Atom Size 28) = 36 pos = torch . rand ( 2 ) * ( 64 - 28 ) # 2. Random Angle (Theta) # range: [0, 2*PI] angle = torch . rand ( 1 ) * 2 * 3.14159 # 3. Calculate Velocity Vector # Decompose angle into x and y components vel = torch . tensor ([ torch . cos ( angle ), torch . sin ( angle )]) Step 4: The Laws of Physics (The Simulation Loop) This is the most critical part. We loop through time ( \\(t = 0\\) to \\(t = 15\\) ) and update the state of the world. Universal Law 1: Momentum Objects move in a straight line unless acted upon. \\[ P_{new} = P_{old} + V \\times \\text{speed} \\tag{10} \\] Universal Law 2: Reflection If an object hits a wall, its velocity in that direction flips sign. If \\(x < 0\\) or \\(x > \\text{limit}\\) : \\(\\quad v_x = -v_x\\) If \\(y < 0\\) or \\(y > \\text{limit}\\) : \\(\\quad v_y = -v_y\\) physics_engine.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # --------------------------------------------------------- # SIMULATION LOOP (t=0 to t=15) # --------------------------------------------------------- for t in range ( seq_len ): # 1. Update Position (Law of Momentum) # Move object along vector by speed factor pos += vel * 3.0 # Speed: 3 pixels per frame # 2. Check Walls (Law of Reflection) # Boundary Limit: 64 (Universe) - 28 (Atom) = 36 # Check Horizontal Walls (Left/Right) if pos [ 0 ] < 0 or pos [ 0 ] > 36 : vel [ 0 ] *= - 1 # Flip X velocity # Check Vertical Walls (Top/Bottom) if pos [ 1 ] < 0 or pos [ 1 ] > 36 : vel [ 1 ] *= - 1 # Flip Y velocity Step 5: Rendering (Occlusion) We know where the digit is (e.g., \\(x = 10.5, y = 5.2\\) ). But computers usually work with integers for array slicing. Discretization: Round the float coordinates to the nearest integer \\((r, c)\\) . Slicing: We copy the \\(28 \\times 28\\) digit image into the \\(64 \\times 64\\) canvas at those coordinates. Occlusion (The \"Over\" Operator): What if two digits overlap? In the real world, the front object blocks the back object. In Moving MNIST, we usually use the MAX operator. If pixel A is white and pixel B is black, the result is white. This simulates \"bright light objects.\" \\[ \\text{Canvas}[r : r + 28, c : c + 28] = \\max(\\text{Canvas}[...], \\text{Digit}) \\tag{11} \\] renderer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # --------------------------------------------------------- # RENDER FRAME # --------------------------------------------------------- # Determine integer coordinates for array slicing r = int ( round ( pos [ 1 ])) # Row (y) c = int ( round ( pos [ 0 ])) # Column (x) # Define the slice window on the canvas # canvas_slice = canvas[r : r+28, c : c+28] # Apply the MAX operator for occlusion # This effectively superimposes the digit onto the canvas canvas [ r : r + 28 , c : c + 28 ] = torch . max ( canvas [ r : r + 28 , c : c + 28 ], digit_image ) Complete Implementation The Assemblage: MicroWorldFactory We now combine the atoms, the void, and the laws of physics into a single, reusable class. micro_world.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 import torch import torch.nn as nn import numpy as np from torchvision.datasets import MNIST class MicroWorldFactory : def __init__ ( self , batch_size = 16 , seq_len = 16 ): self . batch_size = batch_size self . seq_len = seq_len self . img_size = 64 self . digit_size = 28 # ----------------------------------------------------- # HARVEST ATOMS (Load MNIST) # ----------------------------------------------------- print ( \"Loading MNIST atoms...\" ) mnist = MNIST ( root = './data' , train = True , download = True ) self . atoms = mnist . data . float () / 255.0 self . num_atoms = len ( self . atoms ) def create_universe ( self ): \"\"\" Creates a batch of videos with proper physics. Returns: [Batch, Channels, Time, Height, Width] Range: [-1, 1] \"\"\" # Initialize Void: [B, T, C, H, W] # Note: We keep Channels at dim 2 temporarily for easier slicing videos = torch . zeros ( self . batch_size , self . seq_len , 1 , self . img_size , self . img_size ) for b in range ( self . batch_size ): # 1. Select 2 random digits idx1 = np . random . randint ( self . num_atoms ) idx2 = np . random . randint ( self . num_atoms ) digit1 = self . atoms [ idx1 ] digit2 = self . atoms [ idx2 ] # 2. Initialize Physics (Digit 1) pos1 = torch . rand ( 2 ) * ( self . img_size - self . digit_size ) angle1 = torch . rand ( 1 ) * 2 * np . pi vel1 = torch . tensor ([ torch . cos ( angle1 ), torch . sin ( angle1 )]) . squeeze () # 3. Initialize Physics (Digit 2) pos2 = torch . rand ( 2 ) * ( self . img_size - self . digit_size ) angle2 = torch . rand ( 1 ) * 2 * np . pi vel2 = torch . tensor ([ torch . cos ( angle2 ), torch . sin ( angle2 )]) . squeeze () # ------------------------------------------------- # SIMULATION LOOP # ------------------------------------------------- for t in range ( self . seq_len ): # --- RENDER DIGIT 1 --- r1 , c1 = pos1 . round () . int () . tolist () # Clamp to ensure we stay within array bounds r1 = max ( 0 , min ( r1 , self . img_size - self . digit_size )) c1 = max ( 0 , min ( c1 , self . img_size - self . digit_size )) # Apply MAX operator (Occlusion) videos [ b , t , 0 , r1 : r1 + 28 , c1 : c1 + 28 ] = torch . max ( videos [ b , t , 0 , r1 : r1 + 28 , c1 : c1 + 28 ], digit1 ) # --- RENDER DIGIT 2 --- r2 , c2 = pos2 . round () . int () . tolist () r2 = max ( 0 , min ( r2 , self . img_size - self . digit_size )) c2 = max ( 0 , min ( c2 , self . img_size - self . digit_size )) videos [ b , t , 0 , r2 : r2 + 28 , c2 : c2 + 28 ] = torch . max ( videos [ b , t , 0 , r2 : r2 + 28 , c2 : c2 + 28 ], digit2 ) # --- UPDATE PHYSICS (Digit 1) --- pos1 += vel1 * 3.0 # Bounce X if pos1 [ 0 ] < 0 or pos1 [ 0 ] > ( self . img_size - self . digit_size ): vel1 [ 0 ] *= - 1 pos1 [ 0 ] = torch . clamp ( pos1 [ 0 ], 0 , self . img_size - self . digit_size ) # Bounce Y if pos1 [ 1 ] < 0 or pos1 [ 1 ] > ( self . img_size - self . digit_size ): vel1 [ 1 ] *= - 1 pos1 [ 1 ] = torch . clamp ( pos1 [ 1 ], 0 , self . img_size - self . digit_size ) # --- UPDATE PHYSICS (Digit 2) --- pos2 += vel2 * 3.0 # Bounce X if pos2 [ 0 ] < 0 or pos2 [ 0 ] > ( self . img_size - self . digit_size ): vel2 [ 0 ] *= - 1 pos2 [ 0 ] = torch . clamp ( pos2 [ 0 ], 0 , self . img_size - self . digit_size ) # Bounce Y if pos2 [ 1 ] < 0 or pos2 [ 1 ] > ( self . img_size - self . digit_size ): vel2 [ 1 ] *= - 1 pos2 [ 1 ] = torch . clamp ( pos2 [ 1 ], 0 , self . img_size - self . digit_size ) # ----------------------------------------------------- # FORMAT OUTPUT # ----------------------------------------------------- # Permute to Standard Video Tensor: [B, C, T, H, W] # Normalize 0..1 to -1..1 return ( videos . permute ( 0 , 2 , 1 , 3 , 4 ) - 0.5 ) * 2.0 Exercise 1: The \"Teleportation\" Catastrophe (Speed) Let's break the universe we just created. The Goal: Observe the limitations of discrete time steps. The Change: Go to the physics loop in create_universe . Find where we update pos1 . Change the speed modifier from 3.0 to 20.0 . Modification # OLD CODE # pos1 += vel1 * 3.0 # NEW CODE (The Catastrophe) pos1 += vel1 * 20.0 In frame 1, the digit is on the left. In frame 3, it is suddenly on the right. In frame 3, it\u2019s gone (or bounced back instantly). The AI is trying to predict the future. If an object teleports, there is no pattern to learn. The AI will just give up and predict \u201dblur\u201d everywhere because it has no idea where the object went. Exercise 2: The \"Ghost\" Catastrophe (Occlusion) Now, let's break the visual physics. The Goal: Understand the difference between emission (light) and occlusion (matter). The Change: First, restore the speed to 3.0 . Now, find the rendering line with torch.max . Change it to + (addition). Modification # OLD CODE (Correct Occlusion) # videos[...] = torch.max(videos[...], digit1) # NEW CODE (The Ghost Bug) # We simply add the pixel values together videos [ ... ] = videos [ ... ] + digit1 The \u201dGhost\u201d Catastrophe (Occlusion) Exercise 3: The \"Frozen Time\" Catastrophe (Static) Finally, let's break time itself. The Goal: Understand the danger of trivial solutions. The Change: Restore torch.max . Now, change the speed to 0.0 . Modification # OLD CODE # pos1 += vel1 * 3.0 # NEW CODE (The Time Freeze) pos1 += vel1 * 0.0 The \u201dFrozen Time\u201d Phase 2: Building the Brain (Neural Network Architecture) Phase 2 is where we build the \"brain\" (the Neural Network) that will observe the \"universe\" you built in Phase 1. The Blueprint We are building a V-JEPA (Video Joint Embedding Predictive Architecture) . This sounds complex, but it is effectively just three simple LEGO blocks snapped together: The Encoder (The Eye): Looks at video and converts pixels into numbers (embeddings). The Masking (The Game): We hide parts of the video to make the task difficult. The Predictor (The Brain): Tries to guess the missing parts based on what it can see. The Encoder: The Eye (Tubelet Embedding) Standard images are 2D. Videos are 3D (Time + Height + Width). We cannot feed raw pixels directly to a Transformer. We must chop the video into little cubes. These cubes are called Tubelets . The Logic: Video Size: \\(16 \\text{ frames} \\times 64 \\text{ height} \\times 64 \\text{ width}\\) Tubelet Size: \\(1 \\text{ frames} \\times 4 \\text{ height} \\times 4 \\text{ width}\\) The Tubelet Math How many tokens does this generate? Time: \\(16 \\div 1 = 16 \\text{ slices}\\) Height: \\(64 \\div 4 = 16 \\text{ slices}\\) Width: \\(64 \\div 4 = 16 \\text{ slices}\\) Total Tokens: \\(16 \\times 16 \\times 16 = \\mathbf{4096 \\text{ little cubes}}\\) The Code (PyTorch) We use Conv3d . This is a 3D scanner that slides over the video. Kernel: The size of the scanner (the tubelet). Stride: How much the scanner moves. We set stride = kernel so the cubes don't overlap. encoder.py 1 2 3 4 5 6 7 8 9 10 11 12 class TubeletEmbedding ( nn . Module ): def __init__ ( self , embed_dim = 384 ): super () . __init__ () self . proj = nn . Conv3d ( in_channels = 1 , out_channels = embed_dim , kernel_size = ( 1 , 4 , 4 ), stride = ( 1 , 4 , 4 ) ) def forward ( self , x ): return self . proj ( x ) Positional Embeddings: The GPS Transformers are \"blind\" to order. If you shuffle the video cubes, the Transformer doesn't know the difference. We must give each cube a GPS coordinate. Innovation: Factorized 3D Embeddings Instead of giving one ID number (e.g., \"Cube #45\"), we give three coordinates : Time: \"Time = 3\" Row: \"Row = 2\" Column: \"Column = 5\" This teaches the model physics: \"Time 3\" is logically close to \"Time 4,\" whereas \"Cube #45\" has no numerical relation to \"Cube #46\" in a flattened list. The Code We create three separate lookup tables (learnable parameters) and add them together to form the final coordinate. gps_module.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class PositionalEmbedding ( nn . Module ): def __init__ ( self , embed_dim = 384 , max_time_steps = 32 , spatial_size = 16 ): super () . __init__ () self . time_embed = nn . Parameter ( torch . zeros ( 1 , embed_dim , max_time_steps , 1 , 1 )) self . h_embed = nn . Parameter ( torch . zeros ( 1 , embed_dim , 1 , spatial_size , 1 )) self . w_embed = nn . Parameter ( torch . zeros ( 1 , embed_dim , 1 , 1 , spatial_size )) nn . init . normal_ ( self . time_embed , std = 0.02 ) nn . init . normal_ ( self . h_embed , std = 0.02 ) nn . init . normal_ ( self . w_embed , std = 0.02 ) def forward ( self , x ): B , D , T , H , W = x . shape return x + self . time_embed [:, :, : T , :, :] + self . h_embed + self . w_embed The Full Encoder (Eye + GPS + Brain Cells) Now we stack standard Transformer layers on top. This is the \"Context Encoder.\" It combines the \"Eye\" (Tubelet Embeddings), the \"GPS\" (Positional Embeddings), and the \"Brain Cells\" (Attention Blocks) into a single processing unit. context_encoder.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class VideoEncoder ( nn . Module ): def __init__ ( self , embed_dim = 384 , num_layers = 6 ): super () . __init__ () self . embed_dim = embed_dim self . tubelet_embed = TubeletEmbedding ( embed_dim ) self . pos_embed = PositionalEmbedding ( embed_dim ) layer = nn . TransformerEncoderLayer ( d_model = embed_dim , nhead = 8 , dim_feedforward = 1024 , batch_first = True , norm_first = True , dropout = 0.1 ) self . blocks = nn . TransformerEncoder ( layer , num_layers = num_layers ) self . norm = nn . LayerNorm ( embed_dim ) def forward ( self , x ): x = self . tubelet_embed ( x ) x = self . pos_embed ( x ) B , D , T , H , W = x . shape x = x . flatten ( 2 ) . transpose ( 1 , 2 ) x = self . blocks ( x ) x = self . norm ( x ) return x The Masking: The Game This is critical. We randomly hide parts of the video to create a self-supervised learning task. The Rules Context: The parts we show the model (The Clues). Target: The parts we hide and ask the model to guess (The Objective). The Strategy: We use random masking for simplicity in this guide. This means we flip a coin for every tubelet to decide if it is visible or hidden. Production: Hard Mode In a full-scale production environment (like the real V-JEPA paper), they use \"Block Masking\" . This involves hiding large, contiguous chunks of space-time (e.g., covering the entire right half of the video for 5 frames). This prevents the model from \"cheating\" by just interpolating from neighboring pixels and forces it to understand object permanence. The Predictor: The \"Dreamer\" This is the hardest part to understand conceptually. The Predictor receives the Context (what it saw) and must output the Target (what was hidden). The Challenge How does the Predictor know which part was hidden? We cannot just give it a blank void. We must give it a Mask Token (a learnable placeholder) combined with the Target GPS (positional embedding). The Analogy To understand the flow, imagine a conversation between the components: internal_monologue.log Encoder (The Eye): \"I see a ball at Time 1, Position A .\" Predictor (The Dreamer) receives: \"Here is the embedding for the ball at Time 1. Plus a [BLANK TILE] at Time 2, Position B . Fill in the blank.\" Predictor thinks: \"If the ball was at A at Time 1... physics says it must be at B at Time 2. I will fill the blank with 'Ball' .\" VideoPredictor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class VideoPredictor ( nn . Module ): def __init__ ( self , embed_dim = 384 , num_layers = 4 ): super () . __init__ () self . embed_dim = embed_dim self . mask_token = nn . Parameter ( torch . zeros ( 1 , 1 , embed_dim )) nn . init . normal_ ( self . mask_token , std = 0.02 ) self . pos_embed = PositionalEmbedding ( embed_dim ) layer = nn . TransformerEncoderLayer ( d_model = embed_dim , nhead = 8 , dim_feedforward = 1024 , batch_first = True , norm_first = True , dropout = 0.1 ) self . blocks = nn . TransformerEncoder ( layer , num_layers = num_layers ) self . norm = nn . LayerNorm ( embed_dim ) self . pred_head = nn . Linear ( embed_dim , embed_dim ) def forward ( self , context_tokens , context_idx , target_idx , num_tokens ): B = context_tokens . shape [ 0 ] D = self . embed_dim T = num_tokens // ( 16 * 16 ) time_embed_sliced = self . pos_embed . time_embed [:, :, : T , :, :] full_pos = time_embed_sliced + self . pos_embed . h_embed + self . pos_embed . w_embed full_pos = full_pos . flatten ( 2 ) . transpose ( 1 , 2 ) . expand ( B , - 1 , - 1 ) context_pos = torch . gather ( full_pos , 1 , context_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , D )) context_input = context_tokens + context_pos mask_tokens = self . mask_token . expand ( B , target_idx . shape [ 1 ], - 1 ) target_pos = torch . gather ( full_pos , 1 , target_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , D )) mask_input = mask_tokens + target_pos combined = torch . cat ([ context_input , mask_input ], dim = 1 ) x = self . blocks ( combined ) x = self . norm ( x ) return self . pred_head ( x [:, - target_idx . shape [ 1 ]:]) Checkpoint: Understanding the Flow Before we proceed, let's lock in the conceptual pipeline. The Logic Loop Do you see the big picture? Phase 1 gave us the raw video (The Ground Truth). Phase 2 Encoder turns raw video into \"meaningful vectors\" (Embeddings). Phase 2 Masking hides some of these vectors (The Challenge). Phase 2 Predictor takes the visible vectors + \"Mask Placeholders\" and tries to hallucinate the hidden vectors. The Goal: If the predictor succeeds, it means it \"understands\" the physics of the video. (e.g., If it can accurately guess where the ball went without seeing it, it must understand velocity). [Phase 1: Raw Video] | | (Input) v [Phase 2: Encoder] | | (All Embeddings) v {Masking} ------------------------. | | | (Visible Context) | (Hidden Target) v v [Phase 2: Predictor] <========= [Target Vectors] ^ ^ | | (Mask Placeholders) (Loss Function) | | [Predicted Vectors] ==================' | '--> (If match is good, Physics is learned) Trainer trainer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def get_lr_schedule ( step , total_steps , base_lr , final_lr , warmup_steps ): if step < warmup_steps : return base_lr * ( step / warmup_steps ) progress = ( step - warmup_steps ) / ( total_steps - warmup_steps ) cosine_decay = 0.5 * ( 1 + math . cos ( math . pi * progress )) return final_lr + ( base_lr - final_lr ) * cosine_decay def train_world_model ( batch_size = 32 , epochs = 200 , base_lr = 2e-4 , final_lr = 1e-6 , device = 'cuda' ): print ( \"=\" * 50 ) print ( \"STAGE 1: TRAINING V-JEPA\" ) print ( \"=\" * 50 ) factory = MicroWorldFactory ( batch_size = batch_size ) device = torch . device ( device if torch . cuda . is_available () else \"cpu\" ) student_enc = VideoEncoder () . to ( device ) student_pred = VideoPredictor () . to ( device ) teacher_enc = copy . deepcopy ( student_enc ) for p in teacher_enc . parameters (): p . requires_grad = False params = list ( student_enc . parameters ()) + list ( student_pred . parameters ()) optimizer = optim . AdamW ( params , lr = base_lr , weight_decay = 0.05 ) loss_fn = nn . MSELoss () loss_history = [] num_tokens = 16 * 16 * 16 for epoch in range ( epochs ): videos = factory . create_universe () . to ( device ) n_context = int ( num_tokens * 0.1 ) perm = torch . randperm ( num_tokens ) . to ( device ) context_idx = perm [: n_context ] . unsqueeze ( 0 ) . expand ( batch_size , - 1 ) target_idx = perm [ n_context :] . unsqueeze ( 0 ) . expand ( batch_size , - 1 ) lr = get_lr_schedule ( epoch , epochs , base_lr , final_lr , 10 ) for g in optimizer . param_groups : g [ 'lr' ] = lr with torch . no_grad (): teacher_out = teacher_enc ( videos ) targets = torch . gather ( teacher_out , 1 , target_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , 384 )) targets = ( targets - targets . mean ( dim =- 1 , keepdim = True )) / ( targets . std ( dim =- 1 , keepdim = True ) + 1e-6 ) student_full = student_enc ( videos ) context_tokens = torch . gather ( student_full , 1 , context_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , 384 )) predictions = student_pred ( context_tokens , context_idx , target_idx , num_tokens ) loss = loss_fn ( predictions , targets ) optimizer . zero_grad () loss . backward () torch . nn . utils . clip_grad_norm_ ( params , 1.0 ) optimizer . step () ema_decay = 0.996 + ( 0.9995 - 0.996 ) * ( epoch / epochs ) with torch . no_grad (): for t_p , s_p in zip ( teacher_enc . parameters (), student_enc . parameters ()): t_p . data . mul_ ( ema_decay ) . add_ ( s_p . data , alpha = 1 - ema_decay ) loss_history . append ( loss . item ()) if epoch % 10 == 0 : print ( f \"Epoch { epoch } / { epochs } | Loss: { loss . item () : .5f } | LR: { lr : .2e } \" ) return student_enc , teacher_enc Congratulations! You now have a trained V-JEPA. But we have a problem: We cannot \u201csee\u201d what the model is thinking. The model outputs a list of abstract vectors (embeddings). It doesn\u2019t output an image. How do we prove it actually understands physics? We give it a Final Exam. We will use a technique called Linear Probing. Freeze the Brain: We lock the weights of your trained model. It is not allowed to learn anymore. The Question: We ask it: \u201cBased on these numbers you are thinking, where is the digit?\u201d The Test: We train a tiny, single-layer neural network (a Linear Layer) to answer this. If this tiny layer can extract the correct (x, y) coordinates from your embeddings, it proves your model has successfully learned the concept of \u201cPosition\u201d and \u201cVelocity\u201d purely from watching videos. Before running the full exam, let\u2019s look at the \u201cEye\u201d of the model (the first Convolutional layer). Good Result: The filters look like gradients, edges, or moving blobs. Bad Result: The filters look like random TV static (noise). Run this code to inspect your trained model: Visualizing the Learned Eye How do we know the model is actually learning? One of the best sanity checks in Deep Learning is to inspect the first-layer filters . If the filters look like random static (noise), the model hasn't learned anything. If they look like edges, curves, or textures (Gabor filters), the model is beginning to \"see.\" neuroscope.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt def visualize_filters ( encoder ): if hasattr ( encoder , 'patch_embed' ): weights = encoder . patch_embed . weight . data . cpu () elif hasattr ( encoder , 'embed' ) and hasattr ( encoder . embed , 'proj' ): weights = encoder . embed . proj . weight . data . cpu () else : print ( \"Could not find the Conv3d layer. Printing structure:\" ) print ( encoder ) return print ( f \"Visualizing filters from layer shape: { weights . shape } \" ) fig , axes = plt . subplots ( 2 , 8 , figsize = ( 16 , 4 )) for i , ax in enumerate ( axes . flat ): if i >= weights . shape [ 0 ]: break f = weights [ i , 0 , 0 , :, :] ax . imshow ( f , cmap = 'viridis' ) ax . axis ( 'off' ) plt . suptitle ( \"First 16 Filters (The 'Eye' of the Model)\" ) plt . show () # Usage Check: # visualize_filters(student_enc) The Final Exam (Linear Probe Code) We need to modify our Factory to give us the Answers (the actual coordinates) so we can grade the exam. What is Linear Probing? Linear Probing is the standard way to evaluate Self-Supervised Learning (SSL) models. Think of it as a Standardized Test for your AI. The Student (Your Encoder): We take your trained World Model and freeze its weights. It is not allowed to learn anymore. It can only \"see\" and describe what it sees. The Exam (The Probe): We attach a tiny, simple neural network (a few linear layers) on top of the frozen Encoder. The Subject: We ask the Probe to solve a specific task using only the descriptions provided by the Student. The Logic If the tiny Probe can predict the exact pixel coordinates of the digit, it proves that the frozen Encoder must have learned the concept of \"Position\" inside its embeddings. If the Encoder output was just random noise, the Probe would fail. The Code Implementation Copy and run this entire block. It will generate a new test set, extract features using your frozen model, and train the probe. Implementation Note This code is a template . Depending on your specific embed_dim , seq_len , or patch_size settings from Phase 2, you may need to tweak the dimension reshaping in extract_features (specifically the .view() shapes). Ensure the input_dim of the probe matches the output dimension of your specific Encoder. linear_probe.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt from torch.utils.data import TensorDataset , DataLoader # --------------------------------------------------------- # 1. GENERATE EXAM QUESTIONS (Labeled Data) # --------------------------------------------------------- def create_labeled_dataset ( n_samples = 2000 , img_size = 64 , digit_size = 28 ): \"\"\" Generate dataset with NORMALIZED position labels \"\"\" # Initialize factory with batch_size=1 for individual generation factory = MicroWorldFactory ( batch_size = 1 ) videos = [] labels = [] print ( f \"Generating { n_samples } labeled videos...\" ) for _ in range ( n_samples ): video = torch . zeros ( 16 , 1 , img_size , img_size ) # Random Physics Initialization pos = torch . rand ( 2 ) * ( img_size - digit_size ) angle = torch . rand ( 1 ) * 2 * np . pi vel = torch . tensor ([ torch . cos ( angle ), torch . sin ( angle )]) . squeeze () digit = factory . atoms [ np . random . randint ( len ( factory . atoms ))] # Simulation Loop (Identical to Factory) for t in range ( 16 ): r , c = pos . round () . int () . tolist () r = max ( 0 , min ( r , img_size - digit_size )) c = max ( 0 , min ( c , img_size - digit_size )) video [ t , 0 , r : r + digit_size , c : c + digit_size ] = digit # Update Physics pos += vel * 3.0 if pos [ 0 ] < 0 or pos [ 0 ] > ( img_size - digit_size ): vel [ 0 ] *= - 1 pos [ 0 ] = torch . clamp ( pos [ 0 ], 0 , img_size - digit_size ) if pos [ 1 ] < 0 or pos [ 1 ] > ( img_size - digit_size ): vel [ 1 ] *= - 1 pos [ 1 ] = torch . clamp ( pos [ 1 ], 0 , img_size - digit_size ) videos . append ( video ) # Label: Center position of the digit at the last frame? # Or trajectory? Here we label the *current* position (t=0 start or t=end?) # NOTE: The loop updates pos *after* drawing. # So 'pos' is the state at t+1. We usually want the final visible state. center_pos = pos + digit_size / 2 normalized_pos = center_pos / img_size labels . append ( normalized_pos ) # Stack and Normalization X = torch . stack ( videos ) . permute ( 0 , 2 , 1 , 3 , 4 ) # [B, C, T, H, W] X = ( X - 0.5 ) * 2.0 y = torch . stack ( labels ) return X , y # --------------------------------------------------------- # 2. THE PROBE (Tiny Neural Network) # --------------------------------------------------------- class MLPProbe ( nn . Module ): \"\"\" Probe to predict normalized positions from embeddings \"\"\" def __init__ ( self , input_dim = 384 , output_dim = 2 ): super () . __init__ () self . net = nn . Sequential ( nn . Linear ( input_dim , 256 ), nn . LayerNorm ( 256 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( 256 , 128 ), nn . LayerNorm ( 128 ), nn . ReLU (), nn . Linear ( 128 , output_dim ), nn . Sigmoid () # Bound output to [0, 1] for normalized coords ) def forward ( self , x ): return self . net ( x ) # --------------------------------------------------------- # 3. FEATURE EXTRACTION (The Frozen Encoder) # --------------------------------------------------------- def extract_features ( encoder , data_tensor , device , embed_dim = 384 , batch_size = 32 ): encoder . eval () loader = DataLoader ( TensorDataset ( data_tensor ), batch_size = batch_size , shuffle = False ) all_features = [] print ( f \"Extracting features from { len ( data_tensor ) } samples...\" ) with torch . no_grad (): for ( batch_x ,) in loader : batch_x = batch_x . to ( device ) # Use only first 15 frames if predicting 16th? # Or use full context. Adapting to input shape: batch_x_partial = batch_x [:, :, : 15 , :, :] # Pass through Frozen Encoder embeddings = encoder ( batch_x_partial ) # --- DIMENSION ADJUSTMENT NEEDED HERE --- # You must reshape the flat embeddings back to 3D to pool them # Example assumption: 7 spatial tokens, 16 temporal? B = batch_x_partial . shape [ 0 ] # Verify your encoder output shape here! # embeddings_3d = embeddings.view(B, 7, 16, 16, embed_dim) # Ideally, we just take the global average or max pool # feats = embeddings.mean(dim=1) # Global Average Pooling # For this specific snippet logic: # We take the features corresponding to the \"last moment\" # (This logic depends heavily on your Positional Embedding order) feats = embeddings . max ( dim = 1 ) . values # [B, D] Simplified Max Pool all_features . append ( feats . cpu ()) return torch . cat ( all_features ) . to ( device ) # --------------------------------------------------------- # 4. EVALUATION LOOP # --------------------------------------------------------- def evaluate_world_model ( encoder , device , embed_dim = 384 ): \"\"\" Evaluate model with proper train/test split and metrics \"\"\" print ( \"=\" * 50 ) print ( \" EVALUATING WORLD MODEL \" ) print ( \"=\" * 50 ) encoder . eval () X , y = create_labeled_dataset ( n_samples = 2000 ) train_size = int ( 0.8 * len ( X )) X_train , y_train = X [: train_size ], y [: train_size ] X_test , y_test = X [ train_size :], y [ train_size :] print ( f \"Train size: { len ( X_train ) } , Test size: { len ( X_test ) } \" ) print ( f \"Position range (normalized): [ { y_train . min () : .3f } , { y_train . max () : .3f } ]\" ) feat_train = extract_features ( encoder , X_train , device , embed_dim ) feat_test = extract_features ( encoder , X_test , device , embed_dim ) y_train = y_train . to ( device ) y_test = y_test . to ( device ) probe = MLPProbe ( input_dim = embed_dim ) . to ( device ) optimizer = optim . AdamW ( probe . parameters (), lr = 1e-3 , weight_decay = 1e-4 ) loss_fn = nn . MSELoss () print ( \" \\n Training probe...\" ) best_test_loss = float ( 'inf' ) for epoch in range ( 1000 ): probe . train () preds = probe ( feat_train ) loss = loss_fn ( preds , y_train ) optimizer . zero_grad () loss . backward () optimizer . step () if epoch % 100 == 0 : probe . eval () with torch . no_grad (): test_preds = probe ( feat_test ) test_loss = loss_fn ( test_preds , y_test ) test_preds_px = test_preds * 64 y_test_px = y_test * 64 pixel_error = torch . mean ( torch . sqrt ( torch . sum (( test_preds_px - y_test_px ) ** 2 , dim = 1 ))) if test_loss < best_test_loss : best_test_loss = test_loss print ( f \"Epoch { epoch } | Test Loss: { test_loss . item () : .6f } | Pixel Error: { pixel_error . item () : .2f } px\" ) print ( f \" \\n Final Test Loss: { best_test_loss : .6f } \" ) return probe , X_test , y_test def visualize_predictions ( encoder , probe , X_test , y_test , device , embed_dim = 384 , n_examples = 3 ): \"\"\" Visualize model predictions vs ground truth \"\"\" encoder . eval () probe . eval () fig , axes = plt . subplots ( 1 , n_examples , figsize = ( 5 * n_examples , 5 )) if n_examples == 1 : axes = [ axes ] for i , ax in enumerate ( axes ): idx = np . random . randint ( len ( X_test )) video = X_test [ idx : idx + 1 ] . to ( device ) true_pos_norm = y_test [ idx ] . cpu () . numpy () true_pos = true_pos_norm * 64 with torch . no_grad (): feat = extract_features ( encoder , video , device , embed_dim , batch_size = 1 ) pred_pos_norm = probe ( feat ) . cpu () . numpy ()[ 0 ] pred_pos = pred_pos_norm * 64 error = np . sqrt ( np . sum (( true_pos - pred_pos ) ** 2 )) last_frame = video [ 0 , 0 , - 1 ] . cpu () . numpy () ax . imshow ( last_frame , cmap = 'gray' , vmin =- 1 , vmax = 1 ) ax . scatter ( true_pos [ 1 ], true_pos [ 0 ], c = 'lime' , s = 200 , label = 'Ground Truth' , marker = 'o' , edgecolors = 'black' , linewidths = 2 ) ax . scatter ( pred_pos [ 1 ], pred_pos [ 0 ], c = 'red' , s = 200 , label = 'Prediction' , marker = 'x' , linewidths = 3 ) ax . plot ([ true_pos [ 1 ], pred_pos [ 1 ]], [ true_pos [ 0 ], pred_pos [ 0 ]], 'r--' , alpha = 0.6 , linewidth = 2 ) ax . set_title ( f \"Error: { error : .2f } px\" ) ax . legend () ax . axis ( 'off' ) plt . tight_layout () plt . show () Phase 4: The Action-Conditioned World Model Now that our AI understands the laws of physics (Phase 1 & 2), we give it a body. We transition from a passive observer to an active agent . 1. The \"Motor Cortex\" (Action Model) We attach a secondary neural network to the frozen Encoder. This is the Action Model . The Function Input: The current \"thought\" (Latent State \\(z_t\\) ) + An Action (Velocity Vector \\(a_t\\) ). Output: The predicted next \"thought\" ( \\(z_{t+1}\\) ). The Goal: It learns the transition function of the universe: $ \\(f(z_t, a_t) \\rightarrow z_{t+1}\\) $ The Architecture: Cross-Attention for Control How do we fuse a \"Thought\" (complex video tokens) with an \"Action\" (simple velocity vector)? We use Cross-Attention . The \"Thought\" is the Query, and the \"Action\" acts as the Key/Value that modulates the thought. motor_cortex.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import torch import torch.nn as nn class ActionEncoder ( nn . Module ): \"\"\" Transforms raw actions (e.g., velocity [vx, vy]) into a high-dimensional vector. \"\"\" def __init__ ( self , action_dim = 2 , embed_dim = 384 ): super () . __init__ () self . net = nn . Sequential ( nn . Linear ( action_dim , 128 ), nn . LayerNorm ( 128 ), nn . ReLU (), nn . Linear ( 128 , 256 ), nn . LayerNorm ( 256 ), nn . ReLU (), nn . Linear ( 256 , embed_dim ) ) def forward ( self , actions ): return self . net ( actions ) class ActionConditionedPredictor ( nn . Module ): \"\"\" The Brain's Simulator: Takes current state tokens and an action, then hallucinates the next state tokens. \"\"\" def __init__ ( self , embed_dim = 384 , num_layers = 4 ): super () . __init__ () self . embed_dim = embed_dim self . action_encoder = ActionEncoder ( action_dim = 2 , embed_dim = embed_dim ) self . cross_attn = nn . MultiheadAttention ( embed_dim , num_heads = 8 , batch_first = True , dropout = 0.1 ) layer = nn . TransformerEncoderLayer ( d_model = embed_dim , nhead = 8 , dim_feedforward = 1024 , batch_first = True , norm_first = True , dropout = 0.1 ) self . dynamics = nn . TransformerEncoder ( layer , num_layers = num_layers ) self . norm = nn . LayerNorm ( embed_dim ) self . pred_head = nn . Linear ( embed_dim , embed_dim ) def forward ( self , state_tokens , actions ): # Embed Action: [Batch, 2] -> [Batch, 1, Embed_Dim] action_embed = self . action_encoder ( actions ) . unsqueeze ( 1 ) state_conditioned , _ = self . cross_attn ( state_tokens , action_embed , action_embed ) state_conditioned = state_tokens + state_conditioned # Predict Next State next_state = self . dynamics ( state_conditioned ) next_state = self . norm ( next_state ) return self . pred_head ( next_state ) The \"Interpreter\" (Position Decoder) The Motor Cortex works entirely in \"dream space\" (embeddings). To plan a path, we need to translate these dreams back into coordinates \\((X, Y)\\) so we can calculate the distance to our goal. Interpreter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class PositionDecoder ( nn . Module ): \"\"\" Reads the 'dream' tokens and extracts the object's predicted position. Used by the planner to check if the dream leads to the goal. \"\"\" def __init__ ( self , embed_dim = 384 ): super () . __init__ () self . net = nn . Sequential ( nn . Linear ( embed_dim , 256 ), nn . LayerNorm ( 256 ), nn . ReLU (), nn . Linear ( 256 , 128 ), nn . LayerNorm ( 128 ), nn . ReLU (), nn . Linear ( 128 , 2 ), nn . Sigmoid () ) def forward ( self , state_tokens ): pooled = state_tokens . max ( dim = 1 ) . values return self . net ( pooled ) The Training Loop We do not need to label data by hand. We can reuse our simulator to self-supervise the agent. Freeze the Eye: The Encoder is now locked. It provides stable representations of the world. Random Actions: We generate videos where we randomly \"push\" the digit around. Prediction: We train the ActionConditionedPredictor to minimize the difference between its predicted latent state and the actual latent state produced by the Encoder for the next frame. Mathematical Objective We minimize a composite loss function \\(\\mathcal{L}_{total}\\) that ensures the agent predicts both the correct abstract concept (Latent State) and the correct physical location (decoded position). \\[ \\mathcal{L}_{total} = \\mathcal{L}_{dynamics} + \\mathcal{L}_{position} \\] 1. Dynamics Loss (Latent Space): \\[\\mathcal{L}_{dynamics} = || z_{t+1}^{pred} - z_{t+1}^{teacher} ||^2\\] 2. Position Loss (Physical Space): To ground the \"dreams\" in reality, we force the decoded position of the predicted state to match the true coordinate \\(p_{t+1}\\) . \\[\\mathcal{L}_{position} = || \\text{Dec}(z_{t+1}^{teacher}) - p_{t+1} ||^2 + 0.5 \\cdot || \\text{Dec}(z_{t+1}^{pred}) - p_{t+1} ||^2\\] trainer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def train_action_predictor ( frozen_encoder , batch_size = 32 , epochs = 100 , lr = 1e-4 , device = 'cuda' ): print ( \"=\" * 50 ) print ( \"STAGE 2: TRAINING ACTION PREDICTOR\" ) print ( \"=\" * 50 ) device = torch . device ( device if torch . cuda . is_available () else \"cpu\" ) frozen_encoder . eval () for p in frozen_encoder . parameters (): p . requires_grad = False world = ControllableMicroWorld ( batch_size = batch_size , seq_len = 16 ) predictor = ActionConditionedPredictor () . to ( device ) pos_decoder = PositionDecoder () . to ( device ) optimizer = optim . AdamW ( list ( predictor . parameters ()) + list ( pos_decoder . parameters ()), lr = lr ) loss_fn = nn . MSELoss () for epoch in range ( epochs ): videos , actions , states = world . create_controlled_universe () videos , actions , states = videos . to ( device ), actions . to ( device ), states . to ( device ) with torch . no_grad (): full_embeds = frozen_encoder ( videos ) B , _ , D = full_embeds . shape time_sep_embeds = full_embeds . view ( B , 16 , 256 , D ) total_loss = 0 for t in range ( 15 ): curr_state_z = time_sep_embeds [:, t ] next_state_z_gt = time_sep_embeds [:, t + 1 ] action_t = actions [:, t ] next_state_z_pred = predictor ( curr_state_z , action_t ) true_pos_next = states [:, t + 1 , : 2 ] pred_pos_from_gt = pos_decoder ( next_state_z_gt ) pred_pos_from_pred = pos_decoder ( next_state_z_pred ) loss_dynamics = loss_fn ( next_state_z_pred , next_state_z_gt ) loss_pos = loss_fn ( pred_pos_from_gt , true_pos_next ) + 0.5 * loss_fn ( pred_pos_from_pred , true_pos_next ) loss = loss_dynamics + loss_pos optimizer . zero_grad () loss . backward () torch . nn . utils . clip_grad_norm_ ( predictor . parameters (), 1.0 ) optimizer . step () total_loss += loss . item () if epoch % 10 == 0 : print ( f \"Epoch { epoch } / { epochs } | Avg Loss: { total_loss / 15 : .5f } \" ) return predictor , pos_decoder The MPC Planner (The Strategist) Once the Motor Cortex is trained, we don't just predict one step; we plan. We use Model Predictive Control (MPC) . The Planning Loop (CEM) We use the Cross-Entropy Method (CEM) , a sampling-based optimization algorithm. Initialize: Create a Gaussian distribution \\(\\mathcal{N}(\\mu, \\sigma)\\) for action sequences. Sample: Draw \\(N=200\\) random action sequences from this distribution. Evaluate: Run these sequences through the predictor to get latent states \\(z_{1:T}\\) . Decode positions and calculate distance to the goal. Select Elites: Pick the top \\(10\\%\\) sequences with the lowest cost. Refit: Update \\(\\mu\\) and \\(\\sigma\\) to match the elites. Repeat: Iterate 5 times to refine the plan. MPC.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class AdvancedMPCPlanner : def __init__ ( self , encoder , predictor , pos_decoder , device = 'cuda' ): self . encoder = encoder self . predictor = predictor self . pos_decoder = pos_decoder self . device = device def encode_frame ( self , frame ): with torch . no_grad (): return self . encoder ( frame ) def plan_cem ( self , start_frame , goal_frame , horizon = 16 , num_samples = 200 , iterations = 5 ): print ( f \"Planning horizon { horizon } ...\" ) self . encoder . eval () self . predictor . eval () self . pos_decoder . eval () start_z = self . encode_frame ( start_frame ) goal_z = self . encode_frame ( goal_frame ) with torch . no_grad (): goal_pos = self . pos_decoder ( goal_z ) mean = torch . zeros ( horizon , 2 , device = self . device ) std = torch . ones ( horizon , 2 , device = self . device ) * 0.5 best_action_seq = None best_cost = float ( 'inf' ) for i in range ( iterations ): noise = torch . randn ( num_samples , horizon , 2 , device = self . device ) actions_batch = mean . unsqueeze ( 0 ) + noise * std . unsqueeze ( 0 ) curr_z = start_z . repeat ( num_samples , 1 , 1 ) cumulative_cost = torch . zeros ( num_samples , device = self . device ) with torch . no_grad (): for t in range ( horizon ): act = actions_batch [:, t ] next_z = self . predictor ( curr_z , act ) pred_pos = self . pos_decoder ( next_z ) dist = torch . norm ( pred_pos - goal_pos , dim = 1 ) if t == horizon - 1 : cumulative_cost += dist * 10.0 else : cumulative_cost += dist curr_z = next_z elite_k = int ( num_samples * 0.1 ) elite_idxs = torch . argsort ( cumulative_cost )[: elite_k ] elites = actions_batch [ elite_idxs ] mean = elites . mean ( dim = 0 ) std = elites . std ( dim = 0 ) + 0.1 current_best_cost = cumulative_cost [ elite_idxs [ 0 ]] . item () if current_best_cost < best_cost : best_cost = current_best_cost best_action_seq = elites [ 0 ] print ( f \" CEM Iter { i } : Best Cost { current_best_cost : .4f } \" ) return best_action_seq , best_cost Execution (The Demo Suite) To verify the system, we implement a simulation harness (simulate_rollout) that manually executes physics using the exact logic from the MicroWorld, and a demo runner that performs open-loop and closed-loop control. demo.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def simulate_rollout ( world_factory , start_state_tuple , action_sequence , device ): \"\"\" Manually simulates physics using the exact logic from MicroWorldFactory. Returns: [Batch, Channel, Time, Height, Width] -> [1, 1, T, 64, 64] \"\"\" ( d1 , p1 , v1 , d2 , p2 , v2 ) = copy . deepcopy ( start_state_tuple ) actions = action_sequence . cpu () horizon = len ( actions ) frames = [] for t in range ( horizon ): # Render frame = torch . zeros ( 1 , 64 , 64 ) r1 , c1 = p1 . round () . int () . tolist () r1 = max ( 0 , min ( r1 , 64 - 28 )) c1 = max ( 0 , min ( c1 , 64 - 28 )) frame [ 0 , r1 : r1 + 28 , c1 : c1 + 28 ] = torch . max ( frame [ 0 , r1 : r1 + 28 , c1 : c1 + 28 ], d1 ) r2 , c2 = p2 . round () . int () . tolist () r2 = max ( 0 , min ( r2 , 64 - 28 )) c2 = max ( 0 , min ( c2 , 64 - 28 )) frame [ 0 , r2 : r2 + 28 , c2 : c2 + 28 ] = torch . max ( frame [ 0 , r2 : r2 + 28 , c2 : c2 + 28 ], d2 ) frames . append (( frame - 0.5 ) * 2.0 ) # Update Physics act = actions [ t ] v1 += act v1 = torch . clamp ( v1 , - 5.0 , 5.0 ) p1 += v1 p2 += v2 # Bounce logic if p1 [ 0 ] < 0 or p1 [ 0 ] > ( 64 - 28 ): v1 [ 0 ] *= - 1 ; p1 [ 0 ] = torch . clamp ( p1 [ 0 ], 0 , 64 - 28 ) if p1 [ 1 ] < 0 or p1 [ 1 ] > ( 64 - 28 ): v1 [ 1 ] *= - 1 ; p1 [ 1 ] = torch . clamp ( p1 [ 1 ], 0 , 64 - 28 ) if p2 [ 0 ] < 0 or p2 [ 0 ] > ( 64 - 28 ): v2 [ 0 ] *= - 1 ; p2 [ 0 ] = torch . clamp ( p2 [ 0 ], 0 , 64 - 28 ) if p2 [ 1 ] < 0 or p2 [ 1 ] > ( 64 - 28 ): v2 [ 1 ] *= - 1 ; p2 [ 1 ] = torch . clamp ( p2 [ 1 ], 0 , 64 - 28 ) # FIXED: Return [1, 1, T, 64, 64] return torch . stack ( frames ) . permute ( 1 , 0 , 2 , 3 ) . unsqueeze ( 0 ) def create_rollout_video ( start_frame , goal_frame , planned_frames , filename = \"rollout.mp4\" ): \"\"\" Generates a side-by-side video: Goal | Planned Execution planned_frames shape: [1, 1, T, 64, 64] \"\"\" frames = planned_frames [ 0 , 0 ] . cpu () . numpy () # [T, 64, 64] goal_img = goal_frame [ 0 , 0 , 0 ] . cpu () . numpy () fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) im1 = ax1 . imshow ( frames [ 0 ], cmap = 'gray' , vmin =- 1 , vmax = 1 ) ax1 . set_title ( \"Planned Execution\" ) ax1 . axis ( 'off' ) ax2 . imshow ( goal_img , cmap = 'gray' , vmin =- 1 , vmax = 1 ) ax2 . set_title ( \"Target Goal\" ) ax2 . axis ( 'off' ) def update ( frame_idx ): im1 . set_array ( frames [ frame_idx ]) return [ im1 ] ani = animation . FuncAnimation ( fig , update , frames = len ( frames ), blit = True ) try : ani . save ( filename , writer = 'ffmpeg' , fps = 5 ) print ( f \"Video saved to { filename } \" ) except : print ( \"FFmpeg not found. Skipping video save.\" ) plt . close () def run_advanced_demo (): device = 'cuda' if torch . cuda . is_available () else 'cpu' # 1. Train Models enc_student , enc_teacher = train_world_model ( epochs = 30 , device = device ) predictor , pos_decoder = train_action_predictor ( enc_teacher , epochs = 30 , device = device ) planner = AdvancedMPCPlanner ( enc_teacher , predictor , pos_decoder , device ) # --------------------------------------------------------- # DEMO 1: Visualize Rollout # --------------------------------------------------------- print ( \" \\n \" + \"=\" * 50 ) print ( \"DEMO 1: VISUALIZING ROLLOUT\" ) print ( \"=\" * 50 ) world = ControllableMicroWorld ( batch_size = 1 , seq_len = 16 ) idx1 = np . random . randint ( world . num_atoms ) idx2 = np . random . randint ( world . num_atoms ) digit1 , digit2 = world . atoms [ idx1 ], world . atoms [ idx2 ] pos1 = torch . rand ( 2 ) * ( 64 - 28 ) pos2 = torch . rand ( 2 ) * ( 64 - 28 ) vel1 = torch . randn ( 2 ) * 2.0 vel2 = torch . randn ( 2 ) * 2.0 start_tuple = ( digit1 , pos1 . clone (), vel1 . clone (), digit2 , pos2 . clone (), vel2 . clone ()) # FIXED: Create start frame manually. Shape [1, 1, 1, 64, 64] start_frame_sim = simulate_rollout ( None , start_tuple , torch . zeros ( 1 , 2 ), device ) . to ( device ) # Generate a dummy goal rand_actions = torch . randn ( 16 , 2 ) goal_seq = simulate_rollout ( None , start_tuple , rand_actions , device ) # FIXED: Slice to get last frame while keeping dims [1, 1, 1, 64, 64] goal_frame = goal_seq [:, :, - 1 :] . to ( device ) # Plan best_actions , _ = planner . plan_cem ( start_frame_sim , goal_frame , horizon = 16 ) # Execute Plan predicted_rollout = simulate_rollout ( None , start_tuple , best_actions , device ) # Create Video create_rollout_video ( start_frame_sim , goal_frame , predicted_rollout , \"demo1_rollout.mp4\" ) # --------------------------------------------------------- # DEMO 2: Harder Goals (30 steps) # --------------------------------------------------------- print ( \" \\n \" + \"=\" * 50 ) print ( \"DEMO 2: LONG HORIZON PLANNING (30 STEPS)\" ) print ( \"=\" * 50 ) long_actions = torch . randn ( 30 , 2 ) long_goal_seq = simulate_rollout ( None , start_tuple , long_actions , device ) long_goal_frame = long_goal_seq [:, :, - 1 :] . to ( device ) best_actions_long , _ = planner . plan_cem ( start_frame_sim , long_goal_frame , horizon = 30 , iterations = 10 ) long_rollout = simulate_rollout ( None , start_tuple , best_actions_long , device ) create_rollout_video ( start_frame_sim , long_goal_frame , long_rollout , \"demo2_long_horizon.mp4\" ) # --------------------------------------------------------- # DEMO 3: Closed-Loop Control # --------------------------------------------------------- print ( \" \\n \" + \"=\" * 50 ) print ( \"DEMO 3: CLOSED-LOOP CONTROL (Receding Horizon)\" ) print ( \"=\" * 50 ) current_tuple = copy . deepcopy ( start_tuple ) current_frame = start_frame_sim executed_frames = [] for step in range ( 10 ): print ( f \"Closed Loop Step { step + 1 } /10\" ) # Plan for horizon 10 actions_plan , _ = planner . plan_cem ( current_frame , goal_frame , horizon = 10 , iterations = 3 , num_samples = 50 ) first_action = actions_plan [ 0 : 1 ] # Physics Step next_seq = simulate_rollout ( None , current_tuple , first_action , device ) next_frame = next_seq . to ( device ) # Shape [1, 1, 1, 64, 64] executed_frames . append ( next_frame ) # Update State Tuple d1 , p1 , v1 , d2 , p2 , v2 = current_tuple v1 += first_action [ 0 ] . cpu () v1 = torch . clamp ( v1 , - 5.0 , 5.0 ) p1 += v1 p2 += v2 current_tuple = ( d1 , p1 , v1 , d2 , p2 , v2 ) current_frame = next_frame print ( \"Closed-loop execution complete.\" ) if __name__ == \"__main__\" : run_advanced_demo () References [ABB+23] Mahmoud Assran, Johann Ball\u00e9, Charles Blondel, J\u00f6rg Bornschein, Mathilde Caron, Rianne M\u00fcller, Mahmoud Assran, Sylvain Gelly, and Gabriel Synnaeve. Self-supervised learning from images with a joint-embedding predictive architecture. CVPR, 2023. PDF Link (Accessed: Jan 5, 2026). [App25] Apple Machine Learning Research. Rethinking JEPA: Compute-efficient video SSL with frozen teachers. 2025. Article Link (Accessed: Jan 5, 2026). [K\u013125] \u0130lyurek K\u0131l\u0131\u00e7. Beyond next-token prediction: Yann LeCun\u2019s JEPA and the quest for AI common sense \u2014 where everything is an abstraction. Medium, 2025. Article Link (Accessed: Jan 5, 2026). [LeC] Yann LeCun. Energy-based learning. PDF Link (Accessed: Jan 5, 2026). [LeC22] Yann LeCun. A path towards autonomous machine intelligence version 0.9.2. OpenReview, 62(1):1\u201362, 2022. [Let24] Malcolm Lett. Critical review of LeCun\u2019s introductory JEPA paper. Medium, 2024. Article Link (Accessed: Jan 5, 2026). [Mina] Emergent Mind. Joint-embedding predictive architectures. Topic Link (Accessed: Jan 5, 2026). [Minb] Emergent Mind. VICReg-based JEPA overview. Topic Link (Accessed: Jan 5, 2026). [Red25a] Reddit r/MachineLearning. Why does BYOL/JEPA like models work? How does EMA prevent model collapse? 2025. Thread Link (Accessed: Jan 5, 2026). [Red25b] Reddit r/singularity. Could someone explain what each of these architectures are that LeCun claims could lead to AGI? 2025. Thread Link (Accessed: Jan 5, 2026). [Sha22] Shaped. Yann LeCun: A path towards autonomous machine intelligence. 2022. Blog Link (Accessed: Jan 5, 2026). [Und25] Cogni Down Under. A new kind of AI is emerging and it\u2019s better than LLMs? Medium, Dec 2025. Article Link (Accessed: Jan 5, 2026). Appendix: Mathematical Formalism of V-JEPA Components This appendix details the mathematical operations underpinning the Video Joint Embedding Predictive Architecture (V-JEPA). We decompose the model into four primary components: The Eye (Encoder), The GPS (Positional Embedding), The Game (Masking Strategy), and The Brain (Predictor). The Eye: Mathematics of 3D Convolution The \"Eye\" functions mathematically as a linear projection from the high-dimensional pixel space to a lower-dimensional feature space. We employ a 3D Convolutional Neural Network (CNN) operation to achieve this transformation. The Input Tensor ( \\(V\\) ) The input video is represented as a 4D tensor (omitting the batch dimension for clarity): \\[ V \\in \\mathbb{R}^{T \\times H \\times W} \\] Where: \\(T=16\\) (Time frames) \\(H=64, W=64\\) (Height and Width in pixels) The value range is normalized: \\(V_{t,y,x} \\in [-1, 1]\\) The Transformation (The Kernel \\(K\\) ) The Conv3d layer learns a set of filters (weights). Assuming an embedding dimension of \\(D=256\\) , we have 256 such filters. Each filter \\(k\\) is a spatiotemporal cube of weights: \\[ W_k \\in \\mathbb{R}^{2 \\times 8 \\times 8} \\] This corresponds to a kernel size covering 2 frames in time and an \\(8 \\times 8\\) pixel patch in space. The Convolution Operation To derive the scalar embedding value for a specific tubelet at grid position \\((t, i, j)\\) and filter \\(k\\) , we compute the dot product between the video pixels and the kernel weights, adding a bias term \\(b_k\\) . This operation is formally a weighted sum: \\[ E_{t,i,j}^{(k)} = \\left( \\sum_{\\tau=0}^{1} \\sum_{y=0}^{7} \\sum_{x=0}^{7} V_{t+\\tau, i+y, j+x} \\cdot W_k(\\tau, y, x) \\right) + b_k \\] The Output (Flattening) Applying this operation across the entire video volume yields a grid of embedding vectors. We flatten this 3D grid into a sequence of vectors, referred to as \"tokens\": \\[ Z = \\{ z_1, z_2, \\dots, z_{512} \\} \\] Where each token \\(z_n \\in \\mathbb{R}^{256}\\) . Summary: The Eye transforms a spatiotemporal block of 128 pixels ( \\(2 \\times 8 \\times 8\\) ) into a single vector of 256 scalars, effectively compressing local visual information (e.g., \"contains a curved line moving right\") into a latent representation. The GPS: Mathematics of Factorized Embeddings Transformers process inputs as sets, not sequences; a set \\(\\{A, B, C\\}\\) is mathematically identical to \\(\\{C, A, B\\}\\) . Without explicit positional information (GPS), the model lacks the inductive bias to understand that Frame 1 temporally precedes Frame 2. The Problem Standard 1D positional embeddings assign a unique vector \\(P_n\\) to the \\(n\\) -th token in a flattened sequence: \\[ z_n' = z_n + P_n \\] However, in a video volume, token \\(n=10\\) might be spatially adjacent to token \\(n=18\\) (in the row below) or temporally adjacent to token \\(n=74\\) (in the next frame). A single scalar index \\(n\\) destroys these 3D relational structures. The Solution (Factorized Addition) To preserve 3D structure, we define three distinct vector spaces for positional embeddings: Time Space: \\(E_T \\in \\mathbb{R}^{8 \\times D}\\) Height Space: \\(E_H \\in \\mathbb{R}^{8 \\times D}\\) Width Space: \\(E_W \\in \\mathbb{R}^{8 \\times D}\\) For a token located at grid coordinates \\((t, h, w)\\) , the positional embedding vector \\(P_{(t,h,w)}\\) is calculated as the element-wise sum of the three component vectors: \\[ P_{(t,h,w)} = E_T[t] + E_H[h] + E_W[w] \\] Why Summation? One might ask: Why not concatenation? Concatenation would triple the dimension of the embedding vector ( \\(3 \\times D\\) ). By using summation, we maintain the vector dimension \\(D\\) . In high-dimensional geometry, the sum of different random vectors results in a new vector that is nearly orthogonal (unique) to the original components. The neural network learns to \"disentangle\" this sum to recover the underlying spatiotemporal coordinates. The Game: Mathematics of Set Partitioning The masking strategy employed in JEPAs is fundamentally a set theory operation designed to create a self-supervised learning signal. The Universe of Indices ( \\(U\\) ) We define \\(U\\) as the set of indices for all tokens in the video: \\[ U = \\{ 1, 2, \\dots, 512 \\} \\] The Partition We randomly partition \\(U\\) into two disjoint subsets: Context Set ( \\(I_{context}\\) ): The indices of visible tokens. Target Set ( \\(I_{target}\\) ): The indices of hidden tokens. This partition satisfies two conditions: \\[ I_{context} \\cap I_{target} = \\emptyset \\] \\[ I_{context} \\cup I_{target} = U \\] The Masking Ratio ( \\(\\rho\\) ) The scalar \\(\\rho\\) defines the difficulty of the task: \\[ |I_{target}| = \\rho \\cdot |U| \\] For example, if \\(\\rho = 0.6\\) , we obscure 60% of the tokens. The Input Construction We construct two distinct inputs for the neural networks based on this partition: Encoder Input ( \\(X_{enc}\\) ): Contains only the subset of vectors \\(Z\\) belonging to the context. \\[ X_{enc} = \\{ z_i + P_i \\mid i \\in I_{context} \\} \\] Predictor Input ( \\(X_{pred}\\) ): A hybrid sequence containing real data from the context and learned placeholders for the targets. \\[ X_{pred} = X_{enc} \\cup \\{ M + P_j \\mid j \\in I_{target} \\} \\] Where \\(M\\) is a learnable \"Mask Token\" vector. Crucial Math: The predictor receives the GPS coordinates ( \\(P_j\\) ) of the missing parts but not their content ( \\(z_j\\) ). This creates the query: \"Given the surrounding context, what content belongs at coordinate \\(P_j\\) ?\" The Brain: Mathematics of Self-Attention The Predictor is a Transformer architecture. Its core mathematical operation is Scaled Dot-Product Attention, which allows the model to route information between tokens. The Input Matrix ( \\(X\\) ) The input to a transformer block is a matrix of size \\(N \\times D\\) , where \\(N\\) is the number of tokens and \\(D\\) is the embedding dimension. Projections We project the input \\(X\\) into three distinct views using learnable weight matrices \\(W_Q, W_K, W_V\\) : Queries ( \\(Q = X W_Q\\) ): Represents what the token is looking for. Keys ( \\(K = X W_K\\) ): Represents what the token contains. Values ( \\(V = X W_V\\) ): Represents the information the token will pass on. The Attention Equation The attention mechanism computes a weighted sum of values based on the similarity between queries and keys: \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{D}} \\right) V \\] Interpretation: \\(QK^T\\) : Calculates a similarity score (dot product) between every pair of tokens. For example, a \"Mask Token at (Time 2, Pos A)\" might generate a high dot product with a \"Context Token at (Time 1, Pos A)\" due to temporal proximity. Softmax: Normalizes these scores into probabilities that sum to 1. This forms the \"attention map.\" \\(\\times V\\) : Computes the weighted average of the Values. If a Mask Token attends 90% to a specific Context Token, it aggregates 90% of that context's information vector. The Optimization Goal (The Loss Function) The objective is to minimize the distance between the Predictor's output for the masked regions ( \\(\\hat{z}\\) ) and the Teacher's output ( \\(y\\) ). We typically utilize the L2 distance (Mean Squared Error): \\[ \\mathcal{L} = \\sum_{j \\in I_{target}} || \\hat{z}_j - y_j ||^2 \\] Where: \\(\\hat{z}_j\\) : The latent vector predicted by the student network (the brain). \\(y_j\\) : The latent vector extracted by the teacher network (the eye) from the full, unmasked video. Gradient Descent: We calculate the gradient of the Loss \\(\\mathcal{L}\\) with respect to the weights ( \\(\\nabla W\\) ) and update the weights to minimize this error. Summary of the Flow Eye: \\(Pixels \\xrightarrow{\\text{Sum \\& Dot Product}} Embeddings\\) GPS: \\(Embeddings \\xrightarrow{\\text{Vector Addition}} Located\\_Embeddings\\) Game: \\(Located\\_Embeddings \\xrightarrow{\\text{Set Split}} Context \\cup Targets\\) Brain: \\(Context \\xrightarrow{\\text{Matrix Multiplication (Attention)}} Predictions\\)","title":"HOME"},{"location":"#the-epistemological-crisis-of-generative-ai","text":"The field of artificial intelligence currently finds itself at a critical turning point. The dominant approach, driven by the enormous success of Large Language Models (LLMs) and systems that generate content automatically, has produced results that are impressively fluent. These systems can use language with a skill that appears similar to human reasoning, yet they remain fundamentally disconnected from the physical reality they claim to describe. This report argues that the current path\u2014making auto-regressive models larger and larger with more parameters and more data\u2014is approaching a limit that falls short of true Autonomous Machine Intelligence (AMI) [LeC22] . The main problem with the generative approach lies in how it learns: by predicting the next token (word piece) or reconstructing the next pixel. This is a mechanism of approximation, not genuine understanding. When an LLM predicts how a physical object will move, it does not actually simulate the physics of mass and momentum. Instead, it retrieves a statistical pattern of how humans typically describe such movements in text. It works in the separate, symbolic world of language, or the complex, noisy world of pixels, rather than in the continuous, abstract space of underlying reality [Sha22] . To close the gap between statistical imitation and real understanding, we must move away from the goal of generation and toward prediction in latent space. We must build systems that construct an internal World Model\u2014a simulation of the environment that captures meaningful cause and effect while filtering out the irrelevant details of sensory noise. This document serves as both a foundational text and a practical guide for constructing such a system. It explains the theoretical shift from probabilistic models to Energy-Based Models (EBMs), details the Joint-Embedding Predictive Architecture (JEPA), and provides a complete, step-by-step methodology for implementing a Video-JEPA (V-JEPA) using the Moving MNIST dataset as a simplified example of physical reality.","title":"The Epistemological Crisis of Generative AI"},{"location":"#the-auto-regressive-trap-system-1-without-system-2","text":"To understand why we need World Models, we must first identify the problems with current AI systems. Auto-regressive models, such as the Transformer architectures that power GPT-4 or Llama, work in a way similar to \u201dSystem 1\u201d thinking in human psychology\u2014fast, automatic, and reactive [K\u013125] . They generate responses one token (word piece) at a time, with each step based only on what came before. This process is fundamentally unable to plan ahead. Planning requires the ability to imagine multiple possible futures, evaluate how good each outcome would be, and choose the best path before taking the first action. An auto-regressive model, by its very nature, commits to an output immediately. It speaks without thinking first [Let24] . Additionally, applying this generative approach to visual information\u2014predicting the next video frame pixel by pixel\u2014is both computationally expensive and conceptually flawed. The world is complex and filled with random, unpredictable details. The texture of a carpet, the random movement of leaves in the wind, or the exact reflection of light on water are high-entropy details that are largely unpredictable and often irrelevant to the actual task. A generative model trained to reconstruct these pixels must use enormous capacity to model this noise. If it fails to predict the exact texture of a leaf, it receives a high error penalty, even if it correctly predicts where the leaf is generally located. This misalignment of goals forces the model to focus on small, detailed features at the expense of understanding high-level, meaningful dynamics.Claude is AI and can make mistakes. Please double-check responses [Sha22] .","title":"The Auto-Regressive Trap: System 1 without System 2"},{"location":"#the-biological-counter-proof","text":"Biological intelligence provides clear evidence that pixel-level prediction is unnecessary for understanding. A human infant does not learn the physics of the world by predicting the brightness value of every light receptor in their retina. Instead, the infant observes the world and builds an internal abstraction\u2014a model\u2014of object permanence, gravity, occlusion, and inertia. By the age of a few months, long before learning language, an infant possesses \u201dcommon sense\u201d physics. They show surprise when an object passes through a solid wall or when it disappears without explanation. This surprise is the result of a prediction error in their internal World Model. This World Model allows biological agents to perform \u201dSystem 2\u201d thinking: slow, careful reasoning and planning. It enables an agent to imagine the consequences of its actions without risking physical harm. It allows for the simulation of \u201dwhat if\u201d scenarios (\u201dWhat would happen if I dropped this glass?\u201d). The goal of AMI research is to create this capability in artificial systems. We must move from machines that reproduce the surface appearance of the world to machines that understand the underlying structure of the world.","title":"The Biological Counter-Proof"},{"location":"#the-architecture-of-autonomous-machine-intelligence","text":"To replicate the capabilities of biological intelligence, a modular cognitive architecture for AMI was proposed in [LeC22] . This architecture is not a single neural network but a system of interacting components, centered around the World Model. Each module plays a distinct role in the perception-action loop, enabling the agent to reason, plan, and learn from observation (Figure 1). Figure 1: System architecture for autonomous intelligence with differentiable modules: configurator, perception, world model, cost (intrinsic + critic), short-term memory, and actor.","title":"The Architecture of Autonomous Machine Intelligence"},{"location":"#the-six-core-modules","text":"The architecture comprises six distinct modules, each differentiable and trainable, allowing gradients to propagate through the entire system (Table 1). \ud83e\udde9 Module \u2699\ufe0f Function \ud83e\uddec Biological Analogy Configurator Acts as the executive controller. Sets goals and dynamically modulates the parameters of other modules based on the current task. Determines what the agent should do . Prefrontal Cortex (Executive Function) Perception Estimates the current world state s\u209c from sensory input x\u209c . Filters noise and extracts relevant semantic features. Sensory Cortex (Visual / Auditory) World Model Internal simulator. Predicts future states s\u209c\u208a\u2081 from current states s\u209c and hypothetical actions a\u209c . Can infer missing state information. Hippocampus / Frontal Cortex Cost Computes the \u201cenergy\u201d or \u201cdiscomfort\u201d of a state. Combines intrinsic drives (e.g., energy minimization) with extrinsic, task-specific objectives. Amygdala / Basal Ganglia (Reward\u2013Pain System) Actor Proposes candidate action sequences to minimize predicted future cost. Does not act directly\u2014feeds proposals to the World Model for evaluation. Premotor Cortex Short-Term Memory Maintains recent sequences of states, actions, and costs, enabling temporal context for prediction and planning. Working Memory","title":"The Six Core Modules"},{"location":"#the-centrality-of-the-world-model","text":"Among these, the World Model is the most critical and the most challenging to construct. It is the engine of prediction. The World Model must satisfy two competing requirements: Informativeness: The state representation must contain enough information to distinguish between functionally different situations (e.g., the difference between a car moving towards you vs. away from you). Predictability: The state representation must discard information that is unpredictable or irrelevant (e.g., the exact pattern of clouds in the sky, unless the task is weather forecasting). Standard generative models fail the second requirement by trying to predict everything. Purely invariance- based models (like contrastive learning) risk failing the first by collapsing distinct states into identical rep- resentations if the data augmentation views are too aggressive [Und25] . The Joint-Embedding Predictive Architecture (JEPA) is designed specifically to navigate this trade-off.","title":"The Centrality of the World Model"},{"location":"#theoretical-foundations-from-probability-to-energy","text":"The mathematical framework underpinning most current AI is probabilistic modeling. We attempt to estimate \\(P(Y|X)\\) , the probability distribution of the output given the input. The Normalization Constraint While rigorous, this framework imposes a severe constraint: the distribution must normalize (sum to one). In high-dimensional spaces like video, calculating the normalization constant (the partition function ) is intractable. This forces researchers to: Rely on approximations. Restrict the model to simplified distributions (like Gaussians in VAEs) that do not match the complexity of the real world [LeC].","title":"Theoretical Foundations: From Probability to Energy"},{"location":"#energy-based-models-ebms","text":"We advocate for abandoning the probabilistic straitjacket in favor of Energy-Based Models (EBMs) . An EBM does not attempt to model probabilities. Instead, it defines a scalar energy function \\(E(X, Y)\\) that measures the \"compatibility\" between an input \\(X\\) and a potential output \\(Y\\) . Low Energy (Compatible) The pair \\((X, Y)\\) is compatible. \\(Y\\) is a plausible continuation of \\(X\\) . High Energy (Incompatible) The pair \\((X, Y)\\) is incompatible. \\(Y\\) is physically impossible or semantically unrelated to \\(X\\) . Inference in an EBM becomes an optimization problem: finding the \\(Y\\) that minimizes the energy for a given \\(X\\) : \\[ Y^* = \\underset{Y}{\\mathrm{argmin}} \\, E(X, Y) \\] This framework removes the need for normalization, granting us immense flexibility in designing the architecture of the energy function.","title":"Energy-Based Models (EBMs)"},{"location":"#the-collapse-problem-in-self-supervised-learning","text":"The primary challenge in training EBMs, particularly in self-supervised learning (SSL) where we do not have labels, is Collapse . In SSL, we typically have pairs of compatible data points \\((x, y)\\) \u2014for example, two frames from the same video. We want to train the model such that \\(E(x, y)\\) is low.","title":"The Collapse Problem in Self-Supervised Learning"},{"location":"#the-naive-approach","text":"A naive approach would simply minimize the distance between their representations: \\[ L = \\| \\text{Enc}(x) - \\text{Enc}(y) \\|^2 \\tag{2} \\] The Catastrophic Failure Mode The fatal flaw of this objective is that the encoder learns to map every input to a constant vector (e.g., zero). \\[ \\text{Enc}(x) = 0, \\quad \\text{Enc}(y) = 0 \\implies L = 0 \\tag{3} \\] The loss is minimized perfectly, but the representations contain zero information . The energy surface becomes completely flat (zero everywhere), making the model useless for discrimination or planning.","title":"The Naive Approach"},{"location":"#strategies-for-preventing-collapse","text":"To train a useful EBM, we must ensure that the energy is low for compatible pairs (positive samples) and high for incompatible pairs (negative samples). We term this \"shaping the energy landscape.\" There are three primary families of techniques to achieve this [Red25b] :","title":"Strategies for Preventing Collapse"},{"location":"#1-contrastive-methods","text":"Explicitly push up the energy of negative samples. This requires mining \"negative\" pairs\u2014inputs that are definitely unrelated. Pros: Highly effective (e.g., SimCLR). Cons: Inefficient because the space of \"incorrect\" answers is infinite. The model must see a vast number of negatives to define the energy boundary [Mina] .","title":"1. Contrastive Methods"},{"location":"#2-regularization-methods","text":"Impose constraints on the information content of the embeddings. Methods like VICReg (Variance-Invariance-Covariance Regularization) explicitly penalize collapse by adding loss terms: Variance Term: Forces neurons to vary across the batch (prevents constant output). Covariance Term: Forces neurons to capture different features (prevents redundancy).","title":"2. Regularization Methods"},{"location":"#3-architectural-methods-the-jepa-approach","text":"Design the architecture such that the target representation is not fixed but evolves in a way that the predictor cannot easily cheat. This often involves asymmetry , such as using a \"Teacher\" network that is an Exponential Moving Average (EMA) of the \"Student\" network. This creates a moving target that prevents the student from converging to a trivial constant solution [Red25a] .","title":"3. Architectural Methods (The JEPA Approach)"},{"location":"#joint-embedding-predictive-architectures-jepa","text":"The JEPA represents the convergence of World Modeling theory and Energy-Based collapse prevention. Non-Generative by Design JEPA is a non-generative architecture : it does not reconstruct \\(x\\) . Instead, it predicts the latent representation of \\(y\\) from the latent representation of \\(x\\) .","title":"Joint-Embedding Predictive Architectures (JEPA)"},{"location":"#the-core-mechanism","text":"The JEPA consists of three primary sub-networks: Context Encoder (Student): Processes the observed part of the input (the context, \\(x\\) ) to produce a representation \\(s_x\\) . Target Encoder (Teacher): Processes the part of the input to be predicted (the target, \\(y\\) ) to produce a representation \\(s_y\\) . Predictor: A network that takes \\(s_x\\) (and potentially a latent variable \\(z\\) or action \\(a\\) ) and outputs a prediction \\(\\hat{s}_y\\) . The objective is to minimize the distance between the prediction and the target in embedding space: \\[ L = D(\\hat{s}_y, s_y) \\tag{4} \\] Preventing Collapse via EMA Critically, the Target Encoder is not updated via gradient descent from this loss. Doing so would allow the two encoders to conspire to output a constant. Instead, the Target Encoder's weights \\(\\phi\\) are updated as an Exponential Moving Average (EMA) of the Context Encoder's weights \\(\\theta\\) : \\[ \\phi_t \\leftarrow \\tau \\phi_{t-1} + (1 - \\tau)\\theta_t \\tag{5} \\] Where \\(\\tau\\) is a decay parameter typically close to 1 (e.g., 0.996). This asymmetry ensures that the target representation is stable and semantically rich, forcing the predictor to learn the underlying dynamics to match it.","title":"The Core Mechanism"},{"location":"#i-jepa-image-based-world-modeling","text":"I-JEPA applies this principle to static images. It treats the image as a \"world\" where spatial relationships define the physics. Context: A subset of image patches. Target: A different, masked subset of patches. By predicting the embeddings of the missing patches, the model learns high-level semantic features (object parts, shapes) without ever generating pixels. Overcoming Texture Bias This approach avoids the \"texture bias\" of generative models like Masked Autoencoders (MAE) . While MAE wastes capacity reconstructing high-frequency noise (pixels), I-JEPA focuses solely on semantic content by operating in the latent space.","title":"I-JEPA: Image-Based World Modeling"},{"location":"#v-jepa-the-engine-of-physical-understanding","text":"V-JEPA extends this to the temporal domain, which is the natural domain of World Models. In V-JEPA, the input is a video sequence. Context: A set of spatio-temporal blocks (tubelets) from the video. Target: A different set of tubelets, often representing the future frames or occluded regions. Physics as Prediction: By predicting the representation of future frames, the model implicitly learns the laws of physics. To predict where a ball will be in \\(t + 1\\) , it must understand velocity and inertia. To predict the representation of a person walking behind a tree, it must understand occlusion and object permanence [App25] . Crucial: The Masking Strategy The masking strategy in V-JEPA is critical to success. Random Masking (Bad): If we mask random scattered tubelets, the model can simply interpolate from neighbors (spatial smoothing). Block Masking (Good): To force the learning of dynamics, we must mask entire blocks of time or space . This forces the model to bridge the gap using its understanding of motion rather than local texture statistics [ABB+23].","title":"V-JEPA: The Engine of Physical Understanding"},{"location":"#the-challenge-of-stochasticity-and-latent-variables","text":"The real world is not deterministic . Deterministic: If an agent drops a pen, it will fall. Stochastic: If an agent observes a car approaching an intersection, the car might turn left, right, or go straight. The Flaw of Averaging A World Model that outputs a single deterministic prediction \\(\\hat{s}_{t+1}\\) will inevitably predict the average of all possible futures. In embedding space, the average of a \"left-turn representation\" and a \"right-turn representation\" might be a \"crash-into-the-divider representation\" or a blurry, non-descript state.","title":"The Challenge of Stochasticity and Latent Variables"},{"location":"#latent-variable-z","text":"","title":"Latent Variable \\(z\\)"},{"location":"#handling-multimodal-futures","text":"To handle multimodal futures, the JEPA architecture supports a latent variable \\(z\\) . The predictor becomes a function of both the context and this latent variable: \\[ \\hat{s}_y = \\text{Predictor}(s_x, z) \\tag{6} \\] The variable \\(z\\) encodes the information present in the future ( \\(y\\) ) that is not present in the past ( \\(x\\) ). Generative vs. EBM Frameworks The treatment of \\(z\\) differs fundamentally between paradigms: Generative Framework: We would sample \\(z\\) from a fixed prior distribution. EBM Framework: \\(z\\) is an input that minimizes the energy . \\[ E(x, y) = \\min_z D(\\hat{s}_y(z), s_y) \\tag{7} \\] This allows the model to \"explain away\" the uncertainty. For a specific future \\(y\\) (e.g., the car turned left), there exists a \\(z\\) (representing the \"decision to turn left\") that makes the prediction match the target.","title":"Handling Multimodal Futures"},{"location":"#determinism-in-simplified-environments","text":"","title":"Determinism in Simplified Environments"},{"location":"#practical-implementation-moving-mnist","text":"For the practical implementation guide in this report, we will focus on the Moving MNIST dataset. This environment is largely deterministic: Dynamics: Digits move with constant velocity. Physics: Digits reflect off walls deterministically. Ambiguity: There is only minor ambiguity during occlusion (e.g., determining which digit is on top). Simplification Strategy Because the physics are rigid, we can initially simplify our architecture by omitting the explicit latent \\(z\\) . We rely on the inherent capacity of the predictor to approximate the dominant mode of the distribution. This reduces architectural complexity for the learner while still capturing the core principles of the JEPA.","title":"Practical Implementation: Moving MNIST"},{"location":"#practical-implementation-strategy-the-micro-world","text":"We now transition from theory to practice. The request requires a step-by-step guide to building a World Model using a small, open-source dataset. The Selection We select Moving MNIST as the ideal \"Micro-World.\"","title":"Practical Implementation Strategy: The Micro-World"},{"location":"#why-moving-mnist","text":"Moving MNIST consists of sequences (typically 20 frames) of \\(64 \\times 64\\) pixel images containing two handwritten digits bouncing inside a frame. Physics: It exhibits strict physical laws: conservation of momentum (velocity), reflection (bouncing), and depth (occlusion). Manifold Hypothesis: The high-dimensional pixel space ( \\(20 \\times 64 \\times 64 = 81,920\\) dimensions) is generated by a very low-dimensional set of latent variables: the position \\((x, y)\\) , velocity \\((v_x, v_y)\\) , and digit identity ( \\(ID\\) ) for two objects. A successful World Model should theoretically compress the video down to this low-dimensional manifold. Compute Efficiency: Unlike Kinetics-400 or real-world robotics data, Moving MNIST can be trained on a single consumer GPU (or even a high-end CPU in a reasonable time for debugging), making it accessible for a \"from scratch\" guide.","title":"Why Moving MNIST?"},{"location":"#the-data-structure","text":"The dataset is typically generated on-the-fly to prevent overfitting (infinite data regime) or downloaded as a fixed set. Input Tensor: (Batch, Time, Height, Width) Dimensions: (B, 16, 64, 64) (We use 16 frames for standard V-JEPA input). Normalization: Pixels should be normalized to the range [0, 1] or [-1, 1] . Self-Supervised Learning We do not use the class labels (which digit is which) for training. This is Self-Supervised Learning . The signal comes from predicting the masked portions of the video, not from ground-truth labels.","title":"The Data Structure"},{"location":"#phase-1","text":"Phase 1 is the foundation. If the data doesn't obey physics, the World Model has nothing to learn. The Architect We are acting as the \"Creator\" of this Micro-World. We define the space, time, matter, and the laws of physics.","title":"Phase 1"},{"location":"#overview-the-5-step-recipe","text":"We will break the data generation into five logical steps: The Universe: Defining the empty void (Space and Time). The Atoms: Getting the raw matter (MNIST digits). The Big Bang: Setting initial positions and velocities. The Laws of Physics: Updating positions and handling bounces. Rendering: Drawing the \"atoms\" onto the \"universe\" (handling overlap).","title":"Overview: The 5-Step Recipe"},{"location":"#step-1-the-universe-the-5d-tensor","text":"In Deep Learning, a video is just a 5-dimensional block of numbers. The Dimensions: (Batch, Time, Channels, Height, Width) Batch (B): How many parallel universes we simulate at once (e.g., 16 videos). Time (T): The length of history (e.g., 16 frames). Channels (C): 1 (Black & White). Height/Width (H, W): \\(64 \\times 64\\) pixels. void.py 1 2 3 4 5 6 7 8 # --------------------------------------------------------- # INITIALIZE THE VOID # --------------------------------------------------------- # Create an empty universe filled with zeros (black) # Target Shape: [Batch, Time, Channels, Height, Width] # Dimensions: [16, 16, 1, 64, 64] videos = torch . zeros ( batch_size , seq_len , 1 , 64 , 64 )","title":"Step 1: The Universe (The 5D Tensor)"},{"location":"#step-2-the-atoms-the-digits","text":"We need objects. We use standard MNIST handwritten digits. The Problem: Standard MNIST is \\(28 \\times 28\\) pixels. Our world is \\(64 \\times 64\\) . The Preparation: We load the MNIST dataset and keep it in memory. We treat these \\(28 \\times 28\\) grids as our \"solid objects.\" atoms.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 # --------------------------------------------------------- # HARVESTING MATTER # --------------------------------------------------------- # We need raw materials to populate the universe. # Source: Standard MNIST handwritten digits. from torchvision.datasets import MNIST # 1. Download and Load MNIST # Root: Where the raw data is stored locally mnist = MNIST ( root = './data' , train = True , download = True ) # 2. Normalize and Float # Convert integer pixels [0, 255] to float [0.0, 1.0] data = mnist . data . float () / 255.0 print ( f \"Matter Harvested: { data . shape } \" ) # Output: torch.Size([60000, 28, 28])","title":"Step 2: The Atoms (The Digits)"},{"location":"#step-3-the-big-bang-initialization","text":"For every video in the batch, we need to decide where the digits start and where they are going. Position ( \\(P\\) ): A random coordinate \\((x, y)\\) inside the \\(64 \\times 64\\) box. Constraint: We don't want the digit to spawn strictly on the edge, so we limit the spawn area to \\(64 - 28 = 36\\) . Velocity ( \\(V\\) ): How fast and in what direction? We pick a random angle \\(\\theta\\) between \\(0\\) and \\(2\\pi\\) ( \\(360^\\circ\\) ). We convert this angle into a velocity vector \\((v_x, v_y)\\) using trigonometry: \\[ v_x = \\cos(\\theta) \\tag{8} \\] \\[ v_y = \\sin(\\theta) \\tag{9} \\] big_bang.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 # --------------------------------------------------------- # INITIALIZE TRAJECTORIES # --------------------------------------------------------- # Determine initial state vectors for all particles (digits). import math # 1. Random Position (x, y) # Limit spawn range to [0, 36] to prevent clipping at t=0 # (Universe Width 64) - (Atom Size 28) = 36 pos = torch . rand ( 2 ) * ( 64 - 28 ) # 2. Random Angle (Theta) # range: [0, 2*PI] angle = torch . rand ( 1 ) * 2 * 3.14159 # 3. Calculate Velocity Vector # Decompose angle into x and y components vel = torch . tensor ([ torch . cos ( angle ), torch . sin ( angle )])","title":"Step 3: The Big Bang (Initialization)"},{"location":"#step-4-the-laws-of-physics-the-simulation-loop","text":"This is the most critical part. We loop through time ( \\(t = 0\\) to \\(t = 15\\) ) and update the state of the world. Universal Law 1: Momentum Objects move in a straight line unless acted upon. \\[ P_{new} = P_{old} + V \\times \\text{speed} \\tag{10} \\] Universal Law 2: Reflection If an object hits a wall, its velocity in that direction flips sign. If \\(x < 0\\) or \\(x > \\text{limit}\\) : \\(\\quad v_x = -v_x\\) If \\(y < 0\\) or \\(y > \\text{limit}\\) : \\(\\quad v_y = -v_y\\) physics_engine.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 # --------------------------------------------------------- # SIMULATION LOOP (t=0 to t=15) # --------------------------------------------------------- for t in range ( seq_len ): # 1. Update Position (Law of Momentum) # Move object along vector by speed factor pos += vel * 3.0 # Speed: 3 pixels per frame # 2. Check Walls (Law of Reflection) # Boundary Limit: 64 (Universe) - 28 (Atom) = 36 # Check Horizontal Walls (Left/Right) if pos [ 0 ] < 0 or pos [ 0 ] > 36 : vel [ 0 ] *= - 1 # Flip X velocity # Check Vertical Walls (Top/Bottom) if pos [ 1 ] < 0 or pos [ 1 ] > 36 : vel [ 1 ] *= - 1 # Flip Y velocity","title":"Step 4: The Laws of Physics (The Simulation Loop)"},{"location":"#step-5-rendering-occlusion","text":"We know where the digit is (e.g., \\(x = 10.5, y = 5.2\\) ). But computers usually work with integers for array slicing. Discretization: Round the float coordinates to the nearest integer \\((r, c)\\) . Slicing: We copy the \\(28 \\times 28\\) digit image into the \\(64 \\times 64\\) canvas at those coordinates. Occlusion (The \"Over\" Operator): What if two digits overlap? In the real world, the front object blocks the back object. In Moving MNIST, we usually use the MAX operator. If pixel A is white and pixel B is black, the result is white. This simulates \"bright light objects.\" \\[ \\text{Canvas}[r : r + 28, c : c + 28] = \\max(\\text{Canvas}[...], \\text{Digit}) \\tag{11} \\] renderer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 # --------------------------------------------------------- # RENDER FRAME # --------------------------------------------------------- # Determine integer coordinates for array slicing r = int ( round ( pos [ 1 ])) # Row (y) c = int ( round ( pos [ 0 ])) # Column (x) # Define the slice window on the canvas # canvas_slice = canvas[r : r+28, c : c+28] # Apply the MAX operator for occlusion # This effectively superimposes the digit onto the canvas canvas [ r : r + 28 , c : c + 28 ] = torch . max ( canvas [ r : r + 28 , c : c + 28 ], digit_image )","title":"Step 5: Rendering (Occlusion)"},{"location":"#complete-implementation","text":"","title":"Complete Implementation"},{"location":"#the-assemblage-microworldfactory","text":"We now combine the atoms, the void, and the laws of physics into a single, reusable class. micro_world.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 import torch import torch.nn as nn import numpy as np from torchvision.datasets import MNIST class MicroWorldFactory : def __init__ ( self , batch_size = 16 , seq_len = 16 ): self . batch_size = batch_size self . seq_len = seq_len self . img_size = 64 self . digit_size = 28 # ----------------------------------------------------- # HARVEST ATOMS (Load MNIST) # ----------------------------------------------------- print ( \"Loading MNIST atoms...\" ) mnist = MNIST ( root = './data' , train = True , download = True ) self . atoms = mnist . data . float () / 255.0 self . num_atoms = len ( self . atoms ) def create_universe ( self ): \"\"\" Creates a batch of videos with proper physics. Returns: [Batch, Channels, Time, Height, Width] Range: [-1, 1] \"\"\" # Initialize Void: [B, T, C, H, W] # Note: We keep Channels at dim 2 temporarily for easier slicing videos = torch . zeros ( self . batch_size , self . seq_len , 1 , self . img_size , self . img_size ) for b in range ( self . batch_size ): # 1. Select 2 random digits idx1 = np . random . randint ( self . num_atoms ) idx2 = np . random . randint ( self . num_atoms ) digit1 = self . atoms [ idx1 ] digit2 = self . atoms [ idx2 ] # 2. Initialize Physics (Digit 1) pos1 = torch . rand ( 2 ) * ( self . img_size - self . digit_size ) angle1 = torch . rand ( 1 ) * 2 * np . pi vel1 = torch . tensor ([ torch . cos ( angle1 ), torch . sin ( angle1 )]) . squeeze () # 3. Initialize Physics (Digit 2) pos2 = torch . rand ( 2 ) * ( self . img_size - self . digit_size ) angle2 = torch . rand ( 1 ) * 2 * np . pi vel2 = torch . tensor ([ torch . cos ( angle2 ), torch . sin ( angle2 )]) . squeeze () # ------------------------------------------------- # SIMULATION LOOP # ------------------------------------------------- for t in range ( self . seq_len ): # --- RENDER DIGIT 1 --- r1 , c1 = pos1 . round () . int () . tolist () # Clamp to ensure we stay within array bounds r1 = max ( 0 , min ( r1 , self . img_size - self . digit_size )) c1 = max ( 0 , min ( c1 , self . img_size - self . digit_size )) # Apply MAX operator (Occlusion) videos [ b , t , 0 , r1 : r1 + 28 , c1 : c1 + 28 ] = torch . max ( videos [ b , t , 0 , r1 : r1 + 28 , c1 : c1 + 28 ], digit1 ) # --- RENDER DIGIT 2 --- r2 , c2 = pos2 . round () . int () . tolist () r2 = max ( 0 , min ( r2 , self . img_size - self . digit_size )) c2 = max ( 0 , min ( c2 , self . img_size - self . digit_size )) videos [ b , t , 0 , r2 : r2 + 28 , c2 : c2 + 28 ] = torch . max ( videos [ b , t , 0 , r2 : r2 + 28 , c2 : c2 + 28 ], digit2 ) # --- UPDATE PHYSICS (Digit 1) --- pos1 += vel1 * 3.0 # Bounce X if pos1 [ 0 ] < 0 or pos1 [ 0 ] > ( self . img_size - self . digit_size ): vel1 [ 0 ] *= - 1 pos1 [ 0 ] = torch . clamp ( pos1 [ 0 ], 0 , self . img_size - self . digit_size ) # Bounce Y if pos1 [ 1 ] < 0 or pos1 [ 1 ] > ( self . img_size - self . digit_size ): vel1 [ 1 ] *= - 1 pos1 [ 1 ] = torch . clamp ( pos1 [ 1 ], 0 , self . img_size - self . digit_size ) # --- UPDATE PHYSICS (Digit 2) --- pos2 += vel2 * 3.0 # Bounce X if pos2 [ 0 ] < 0 or pos2 [ 0 ] > ( self . img_size - self . digit_size ): vel2 [ 0 ] *= - 1 pos2 [ 0 ] = torch . clamp ( pos2 [ 0 ], 0 , self . img_size - self . digit_size ) # Bounce Y if pos2 [ 1 ] < 0 or pos2 [ 1 ] > ( self . img_size - self . digit_size ): vel2 [ 1 ] *= - 1 pos2 [ 1 ] = torch . clamp ( pos2 [ 1 ], 0 , self . img_size - self . digit_size ) # ----------------------------------------------------- # FORMAT OUTPUT # ----------------------------------------------------- # Permute to Standard Video Tensor: [B, C, T, H, W] # Normalize 0..1 to -1..1 return ( videos . permute ( 0 , 2 , 1 , 3 , 4 ) - 0.5 ) * 2.0","title":"The Assemblage: MicroWorldFactory"},{"location":"#exercise-1-the-teleportation-catastrophe-speed","text":"Let's break the universe we just created. The Goal: Observe the limitations of discrete time steps. The Change: Go to the physics loop in create_universe . Find where we update pos1 . Change the speed modifier from 3.0 to 20.0 . Modification # OLD CODE # pos1 += vel1 * 3.0 # NEW CODE (The Catastrophe) pos1 += vel1 * 20.0 In frame 1, the digit is on the left. In frame 3, it is suddenly on the right. In frame 3, it\u2019s gone (or bounced back instantly). The AI is trying to predict the future. If an object teleports, there is no pattern to learn. The AI will just give up and predict \u201dblur\u201d everywhere because it has no idea where the object went.","title":"Exercise 1: The \"Teleportation\" Catastrophe (Speed)"},{"location":"#exercise-2-the-ghost-catastrophe-occlusion","text":"Now, let's break the visual physics. The Goal: Understand the difference between emission (light) and occlusion (matter). The Change: First, restore the speed to 3.0 . Now, find the rendering line with torch.max . Change it to + (addition). Modification # OLD CODE (Correct Occlusion) # videos[...] = torch.max(videos[...], digit1) # NEW CODE (The Ghost Bug) # We simply add the pixel values together videos [ ... ] = videos [ ... ] + digit1 The \u201dGhost\u201d Catastrophe (Occlusion)","title":"Exercise 2: The \"Ghost\" Catastrophe (Occlusion)"},{"location":"#exercise-3-the-frozen-time-catastrophe-static","text":"Finally, let's break time itself. The Goal: Understand the danger of trivial solutions. The Change: Restore torch.max . Now, change the speed to 0.0 . Modification # OLD CODE # pos1 += vel1 * 3.0 # NEW CODE (The Time Freeze) pos1 += vel1 * 0.0 The \u201dFrozen Time\u201d","title":"Exercise 3: The \"Frozen Time\" Catastrophe (Static)"},{"location":"#phase-2-building-the-brain-neural-network-architecture","text":"Phase 2 is where we build the \"brain\" (the Neural Network) that will observe the \"universe\" you built in Phase 1. The Blueprint We are building a V-JEPA (Video Joint Embedding Predictive Architecture) . This sounds complex, but it is effectively just three simple LEGO blocks snapped together: The Encoder (The Eye): Looks at video and converts pixels into numbers (embeddings). The Masking (The Game): We hide parts of the video to make the task difficult. The Predictor (The Brain): Tries to guess the missing parts based on what it can see.","title":"Phase 2: Building the Brain (Neural Network Architecture)"},{"location":"#the-encoder-the-eye-tubelet-embedding","text":"Standard images are 2D. Videos are 3D (Time + Height + Width). We cannot feed raw pixels directly to a Transformer. We must chop the video into little cubes. These cubes are called Tubelets . The Logic: Video Size: \\(16 \\text{ frames} \\times 64 \\text{ height} \\times 64 \\text{ width}\\) Tubelet Size: \\(1 \\text{ frames} \\times 4 \\text{ height} \\times 4 \\text{ width}\\) The Tubelet Math How many tokens does this generate? Time: \\(16 \\div 1 = 16 \\text{ slices}\\) Height: \\(64 \\div 4 = 16 \\text{ slices}\\) Width: \\(64 \\div 4 = 16 \\text{ slices}\\) Total Tokens: \\(16 \\times 16 \\times 16 = \\mathbf{4096 \\text{ little cubes}}\\)","title":"The Encoder: The Eye (Tubelet Embedding)"},{"location":"#the-code-pytorch","text":"We use Conv3d . This is a 3D scanner that slides over the video. Kernel: The size of the scanner (the tubelet). Stride: How much the scanner moves. We set stride = kernel so the cubes don't overlap. encoder.py 1 2 3 4 5 6 7 8 9 10 11 12 class TubeletEmbedding ( nn . Module ): def __init__ ( self , embed_dim = 384 ): super () . __init__ () self . proj = nn . Conv3d ( in_channels = 1 , out_channels = embed_dim , kernel_size = ( 1 , 4 , 4 ), stride = ( 1 , 4 , 4 ) ) def forward ( self , x ): return self . proj ( x )","title":"The Code (PyTorch)"},{"location":"#positional-embeddings-the-gps","text":"Transformers are \"blind\" to order. If you shuffle the video cubes, the Transformer doesn't know the difference. We must give each cube a GPS coordinate. Innovation: Factorized 3D Embeddings Instead of giving one ID number (e.g., \"Cube #45\"), we give three coordinates : Time: \"Time = 3\" Row: \"Row = 2\" Column: \"Column = 5\" This teaches the model physics: \"Time 3\" is logically close to \"Time 4,\" whereas \"Cube #45\" has no numerical relation to \"Cube #46\" in a flattened list.","title":"Positional Embeddings: The GPS"},{"location":"#the-code","text":"We create three separate lookup tables (learnable parameters) and add them together to form the final coordinate. gps_module.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 class PositionalEmbedding ( nn . Module ): def __init__ ( self , embed_dim = 384 , max_time_steps = 32 , spatial_size = 16 ): super () . __init__ () self . time_embed = nn . Parameter ( torch . zeros ( 1 , embed_dim , max_time_steps , 1 , 1 )) self . h_embed = nn . Parameter ( torch . zeros ( 1 , embed_dim , 1 , spatial_size , 1 )) self . w_embed = nn . Parameter ( torch . zeros ( 1 , embed_dim , 1 , 1 , spatial_size )) nn . init . normal_ ( self . time_embed , std = 0.02 ) nn . init . normal_ ( self . h_embed , std = 0.02 ) nn . init . normal_ ( self . w_embed , std = 0.02 ) def forward ( self , x ): B , D , T , H , W = x . shape return x + self . time_embed [:, :, : T , :, :] + self . h_embed + self . w_embed","title":"The Code"},{"location":"#the-full-encoder-eye-gps-brain-cells","text":"Now we stack standard Transformer layers on top. This is the \"Context Encoder.\" It combines the \"Eye\" (Tubelet Embeddings), the \"GPS\" (Positional Embeddings), and the \"Brain Cells\" (Attention Blocks) into a single processing unit. context_encoder.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 class VideoEncoder ( nn . Module ): def __init__ ( self , embed_dim = 384 , num_layers = 6 ): super () . __init__ () self . embed_dim = embed_dim self . tubelet_embed = TubeletEmbedding ( embed_dim ) self . pos_embed = PositionalEmbedding ( embed_dim ) layer = nn . TransformerEncoderLayer ( d_model = embed_dim , nhead = 8 , dim_feedforward = 1024 , batch_first = True , norm_first = True , dropout = 0.1 ) self . blocks = nn . TransformerEncoder ( layer , num_layers = num_layers ) self . norm = nn . LayerNorm ( embed_dim ) def forward ( self , x ): x = self . tubelet_embed ( x ) x = self . pos_embed ( x ) B , D , T , H , W = x . shape x = x . flatten ( 2 ) . transpose ( 1 , 2 ) x = self . blocks ( x ) x = self . norm ( x ) return x","title":"The Full Encoder (Eye + GPS + Brain Cells)"},{"location":"#the-masking-the-game","text":"This is critical. We randomly hide parts of the video to create a self-supervised learning task. The Rules Context: The parts we show the model (The Clues). Target: The parts we hide and ask the model to guess (The Objective). The Strategy: We use random masking for simplicity in this guide. This means we flip a coin for every tubelet to decide if it is visible or hidden. Production: Hard Mode In a full-scale production environment (like the real V-JEPA paper), they use \"Block Masking\" . This involves hiding large, contiguous chunks of space-time (e.g., covering the entire right half of the video for 5 frames). This prevents the model from \"cheating\" by just interpolating from neighboring pixels and forces it to understand object permanence.","title":"The Masking: The Game"},{"location":"#the-predictor-the-dreamer","text":"This is the hardest part to understand conceptually. The Predictor receives the Context (what it saw) and must output the Target (what was hidden). The Challenge How does the Predictor know which part was hidden? We cannot just give it a blank void. We must give it a Mask Token (a learnable placeholder) combined with the Target GPS (positional embedding).","title":"The Predictor: The \"Dreamer\""},{"location":"#the-analogy","text":"To understand the flow, imagine a conversation between the components: internal_monologue.log Encoder (The Eye): \"I see a ball at Time 1, Position A .\" Predictor (The Dreamer) receives: \"Here is the embedding for the ball at Time 1. Plus a [BLANK TILE] at Time 2, Position B . Fill in the blank.\" Predictor thinks: \"If the ball was at A at Time 1... physics says it must be at B at Time 2. I will fill the blank with 'Ball' .\" VideoPredictor.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 class VideoPredictor ( nn . Module ): def __init__ ( self , embed_dim = 384 , num_layers = 4 ): super () . __init__ () self . embed_dim = embed_dim self . mask_token = nn . Parameter ( torch . zeros ( 1 , 1 , embed_dim )) nn . init . normal_ ( self . mask_token , std = 0.02 ) self . pos_embed = PositionalEmbedding ( embed_dim ) layer = nn . TransformerEncoderLayer ( d_model = embed_dim , nhead = 8 , dim_feedforward = 1024 , batch_first = True , norm_first = True , dropout = 0.1 ) self . blocks = nn . TransformerEncoder ( layer , num_layers = num_layers ) self . norm = nn . LayerNorm ( embed_dim ) self . pred_head = nn . Linear ( embed_dim , embed_dim ) def forward ( self , context_tokens , context_idx , target_idx , num_tokens ): B = context_tokens . shape [ 0 ] D = self . embed_dim T = num_tokens // ( 16 * 16 ) time_embed_sliced = self . pos_embed . time_embed [:, :, : T , :, :] full_pos = time_embed_sliced + self . pos_embed . h_embed + self . pos_embed . w_embed full_pos = full_pos . flatten ( 2 ) . transpose ( 1 , 2 ) . expand ( B , - 1 , - 1 ) context_pos = torch . gather ( full_pos , 1 , context_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , D )) context_input = context_tokens + context_pos mask_tokens = self . mask_token . expand ( B , target_idx . shape [ 1 ], - 1 ) target_pos = torch . gather ( full_pos , 1 , target_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , D )) mask_input = mask_tokens + target_pos combined = torch . cat ([ context_input , mask_input ], dim = 1 ) x = self . blocks ( combined ) x = self . norm ( x ) return self . pred_head ( x [:, - target_idx . shape [ 1 ]:])","title":"The Analogy"},{"location":"#checkpoint-understanding-the-flow","text":"Before we proceed, let's lock in the conceptual pipeline. The Logic Loop Do you see the big picture? Phase 1 gave us the raw video (The Ground Truth). Phase 2 Encoder turns raw video into \"meaningful vectors\" (Embeddings). Phase 2 Masking hides some of these vectors (The Challenge). Phase 2 Predictor takes the visible vectors + \"Mask Placeholders\" and tries to hallucinate the hidden vectors. The Goal: If the predictor succeeds, it means it \"understands\" the physics of the video. (e.g., If it can accurately guess where the ball went without seeing it, it must understand velocity). [Phase 1: Raw Video] | | (Input) v [Phase 2: Encoder] | | (All Embeddings) v {Masking} ------------------------. | | | (Visible Context) | (Hidden Target) v v [Phase 2: Predictor] <========= [Target Vectors] ^ ^ | | (Mask Placeholders) (Loss Function) | | [Predicted Vectors] ==================' | '--> (If match is good, Physics is learned)","title":"Checkpoint: Understanding the Flow"},{"location":"#trainer","text":"trainer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 def get_lr_schedule ( step , total_steps , base_lr , final_lr , warmup_steps ): if step < warmup_steps : return base_lr * ( step / warmup_steps ) progress = ( step - warmup_steps ) / ( total_steps - warmup_steps ) cosine_decay = 0.5 * ( 1 + math . cos ( math . pi * progress )) return final_lr + ( base_lr - final_lr ) * cosine_decay def train_world_model ( batch_size = 32 , epochs = 200 , base_lr = 2e-4 , final_lr = 1e-6 , device = 'cuda' ): print ( \"=\" * 50 ) print ( \"STAGE 1: TRAINING V-JEPA\" ) print ( \"=\" * 50 ) factory = MicroWorldFactory ( batch_size = batch_size ) device = torch . device ( device if torch . cuda . is_available () else \"cpu\" ) student_enc = VideoEncoder () . to ( device ) student_pred = VideoPredictor () . to ( device ) teacher_enc = copy . deepcopy ( student_enc ) for p in teacher_enc . parameters (): p . requires_grad = False params = list ( student_enc . parameters ()) + list ( student_pred . parameters ()) optimizer = optim . AdamW ( params , lr = base_lr , weight_decay = 0.05 ) loss_fn = nn . MSELoss () loss_history = [] num_tokens = 16 * 16 * 16 for epoch in range ( epochs ): videos = factory . create_universe () . to ( device ) n_context = int ( num_tokens * 0.1 ) perm = torch . randperm ( num_tokens ) . to ( device ) context_idx = perm [: n_context ] . unsqueeze ( 0 ) . expand ( batch_size , - 1 ) target_idx = perm [ n_context :] . unsqueeze ( 0 ) . expand ( batch_size , - 1 ) lr = get_lr_schedule ( epoch , epochs , base_lr , final_lr , 10 ) for g in optimizer . param_groups : g [ 'lr' ] = lr with torch . no_grad (): teacher_out = teacher_enc ( videos ) targets = torch . gather ( teacher_out , 1 , target_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , 384 )) targets = ( targets - targets . mean ( dim =- 1 , keepdim = True )) / ( targets . std ( dim =- 1 , keepdim = True ) + 1e-6 ) student_full = student_enc ( videos ) context_tokens = torch . gather ( student_full , 1 , context_idx . unsqueeze ( - 1 ) . expand ( - 1 , - 1 , 384 )) predictions = student_pred ( context_tokens , context_idx , target_idx , num_tokens ) loss = loss_fn ( predictions , targets ) optimizer . zero_grad () loss . backward () torch . nn . utils . clip_grad_norm_ ( params , 1.0 ) optimizer . step () ema_decay = 0.996 + ( 0.9995 - 0.996 ) * ( epoch / epochs ) with torch . no_grad (): for t_p , s_p in zip ( teacher_enc . parameters (), student_enc . parameters ()): t_p . data . mul_ ( ema_decay ) . add_ ( s_p . data , alpha = 1 - ema_decay ) loss_history . append ( loss . item ()) if epoch % 10 == 0 : print ( f \"Epoch { epoch } / { epochs } | Loss: { loss . item () : .5f } | LR: { lr : .2e } \" ) return student_enc , teacher_enc","title":"Trainer"},{"location":"#congratulations","text":"You now have a trained V-JEPA. But we have a problem: We cannot \u201csee\u201d what the model is thinking. The model outputs a list of abstract vectors (embeddings). It doesn\u2019t output an image. How do we prove it actually understands physics? We give it a Final Exam. We will use a technique called Linear Probing. Freeze the Brain: We lock the weights of your trained model. It is not allowed to learn anymore. The Question: We ask it: \u201cBased on these numbers you are thinking, where is the digit?\u201d The Test: We train a tiny, single-layer neural network (a Linear Layer) to answer this. If this tiny layer can extract the correct (x, y) coordinates from your embeddings, it proves your model has successfully learned the concept of \u201cPosition\u201d and \u201cVelocity\u201d purely from watching videos. Before running the full exam, let\u2019s look at the \u201cEye\u201d of the model (the first Convolutional layer). Good Result: The filters look like gradients, edges, or moving blobs. Bad Result: The filters look like random TV static (noise). Run this code to inspect your trained model:","title":"Congratulations!"},{"location":"#visualizing-the-learned-eye","text":"How do we know the model is actually learning? One of the best sanity checks in Deep Learning is to inspect the first-layer filters . If the filters look like random static (noise), the model hasn't learned anything. If they look like edges, curves, or textures (Gabor filters), the model is beginning to \"see.\" neuroscope.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 import matplotlib.pyplot as plt def visualize_filters ( encoder ): if hasattr ( encoder , 'patch_embed' ): weights = encoder . patch_embed . weight . data . cpu () elif hasattr ( encoder , 'embed' ) and hasattr ( encoder . embed , 'proj' ): weights = encoder . embed . proj . weight . data . cpu () else : print ( \"Could not find the Conv3d layer. Printing structure:\" ) print ( encoder ) return print ( f \"Visualizing filters from layer shape: { weights . shape } \" ) fig , axes = plt . subplots ( 2 , 8 , figsize = ( 16 , 4 )) for i , ax in enumerate ( axes . flat ): if i >= weights . shape [ 0 ]: break f = weights [ i , 0 , 0 , :, :] ax . imshow ( f , cmap = 'viridis' ) ax . axis ( 'off' ) plt . suptitle ( \"First 16 Filters (The 'Eye' of the Model)\" ) plt . show () # Usage Check: # visualize_filters(student_enc)","title":"Visualizing the Learned Eye"},{"location":"#the-final-exam-linear-probe-code","text":"We need to modify our Factory to give us the Answers (the actual coordinates) so we can grade the exam.","title":"The Final Exam (Linear Probe Code)"},{"location":"#what-is-linear-probing","text":"Linear Probing is the standard way to evaluate Self-Supervised Learning (SSL) models. Think of it as a Standardized Test for your AI. The Student (Your Encoder): We take your trained World Model and freeze its weights. It is not allowed to learn anymore. It can only \"see\" and describe what it sees. The Exam (The Probe): We attach a tiny, simple neural network (a few linear layers) on top of the frozen Encoder. The Subject: We ask the Probe to solve a specific task using only the descriptions provided by the Student. The Logic If the tiny Probe can predict the exact pixel coordinates of the digit, it proves that the frozen Encoder must have learned the concept of \"Position\" inside its embeddings. If the Encoder output was just random noise, the Probe would fail.","title":"What is Linear Probing?"},{"location":"#the-code-implementation","text":"Copy and run this entire block. It will generate a new test set, extract features using your frozen model, and train the probe. Implementation Note This code is a template . Depending on your specific embed_dim , seq_len , or patch_size settings from Phase 2, you may need to tweak the dimension reshaping in extract_features (specifically the .view() shapes). Ensure the input_dim of the probe matches the output dimension of your specific Encoder. linear_probe.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 import torch import torch.nn as nn import torch.optim as optim import numpy as np import matplotlib.pyplot as plt from torch.utils.data import TensorDataset , DataLoader # --------------------------------------------------------- # 1. GENERATE EXAM QUESTIONS (Labeled Data) # --------------------------------------------------------- def create_labeled_dataset ( n_samples = 2000 , img_size = 64 , digit_size = 28 ): \"\"\" Generate dataset with NORMALIZED position labels \"\"\" # Initialize factory with batch_size=1 for individual generation factory = MicroWorldFactory ( batch_size = 1 ) videos = [] labels = [] print ( f \"Generating { n_samples } labeled videos...\" ) for _ in range ( n_samples ): video = torch . zeros ( 16 , 1 , img_size , img_size ) # Random Physics Initialization pos = torch . rand ( 2 ) * ( img_size - digit_size ) angle = torch . rand ( 1 ) * 2 * np . pi vel = torch . tensor ([ torch . cos ( angle ), torch . sin ( angle )]) . squeeze () digit = factory . atoms [ np . random . randint ( len ( factory . atoms ))] # Simulation Loop (Identical to Factory) for t in range ( 16 ): r , c = pos . round () . int () . tolist () r = max ( 0 , min ( r , img_size - digit_size )) c = max ( 0 , min ( c , img_size - digit_size )) video [ t , 0 , r : r + digit_size , c : c + digit_size ] = digit # Update Physics pos += vel * 3.0 if pos [ 0 ] < 0 or pos [ 0 ] > ( img_size - digit_size ): vel [ 0 ] *= - 1 pos [ 0 ] = torch . clamp ( pos [ 0 ], 0 , img_size - digit_size ) if pos [ 1 ] < 0 or pos [ 1 ] > ( img_size - digit_size ): vel [ 1 ] *= - 1 pos [ 1 ] = torch . clamp ( pos [ 1 ], 0 , img_size - digit_size ) videos . append ( video ) # Label: Center position of the digit at the last frame? # Or trajectory? Here we label the *current* position (t=0 start or t=end?) # NOTE: The loop updates pos *after* drawing. # So 'pos' is the state at t+1. We usually want the final visible state. center_pos = pos + digit_size / 2 normalized_pos = center_pos / img_size labels . append ( normalized_pos ) # Stack and Normalization X = torch . stack ( videos ) . permute ( 0 , 2 , 1 , 3 , 4 ) # [B, C, T, H, W] X = ( X - 0.5 ) * 2.0 y = torch . stack ( labels ) return X , y # --------------------------------------------------------- # 2. THE PROBE (Tiny Neural Network) # --------------------------------------------------------- class MLPProbe ( nn . Module ): \"\"\" Probe to predict normalized positions from embeddings \"\"\" def __init__ ( self , input_dim = 384 , output_dim = 2 ): super () . __init__ () self . net = nn . Sequential ( nn . Linear ( input_dim , 256 ), nn . LayerNorm ( 256 ), nn . ReLU (), nn . Dropout ( 0.1 ), nn . Linear ( 256 , 128 ), nn . LayerNorm ( 128 ), nn . ReLU (), nn . Linear ( 128 , output_dim ), nn . Sigmoid () # Bound output to [0, 1] for normalized coords ) def forward ( self , x ): return self . net ( x ) # --------------------------------------------------------- # 3. FEATURE EXTRACTION (The Frozen Encoder) # --------------------------------------------------------- def extract_features ( encoder , data_tensor , device , embed_dim = 384 , batch_size = 32 ): encoder . eval () loader = DataLoader ( TensorDataset ( data_tensor ), batch_size = batch_size , shuffle = False ) all_features = [] print ( f \"Extracting features from { len ( data_tensor ) } samples...\" ) with torch . no_grad (): for ( batch_x ,) in loader : batch_x = batch_x . to ( device ) # Use only first 15 frames if predicting 16th? # Or use full context. Adapting to input shape: batch_x_partial = batch_x [:, :, : 15 , :, :] # Pass through Frozen Encoder embeddings = encoder ( batch_x_partial ) # --- DIMENSION ADJUSTMENT NEEDED HERE --- # You must reshape the flat embeddings back to 3D to pool them # Example assumption: 7 spatial tokens, 16 temporal? B = batch_x_partial . shape [ 0 ] # Verify your encoder output shape here! # embeddings_3d = embeddings.view(B, 7, 16, 16, embed_dim) # Ideally, we just take the global average or max pool # feats = embeddings.mean(dim=1) # Global Average Pooling # For this specific snippet logic: # We take the features corresponding to the \"last moment\" # (This logic depends heavily on your Positional Embedding order) feats = embeddings . max ( dim = 1 ) . values # [B, D] Simplified Max Pool all_features . append ( feats . cpu ()) return torch . cat ( all_features ) . to ( device ) # --------------------------------------------------------- # 4. EVALUATION LOOP # --------------------------------------------------------- def evaluate_world_model ( encoder , device , embed_dim = 384 ): \"\"\" Evaluate model with proper train/test split and metrics \"\"\" print ( \"=\" * 50 ) print ( \" EVALUATING WORLD MODEL \" ) print ( \"=\" * 50 ) encoder . eval () X , y = create_labeled_dataset ( n_samples = 2000 ) train_size = int ( 0.8 * len ( X )) X_train , y_train = X [: train_size ], y [: train_size ] X_test , y_test = X [ train_size :], y [ train_size :] print ( f \"Train size: { len ( X_train ) } , Test size: { len ( X_test ) } \" ) print ( f \"Position range (normalized): [ { y_train . min () : .3f } , { y_train . max () : .3f } ]\" ) feat_train = extract_features ( encoder , X_train , device , embed_dim ) feat_test = extract_features ( encoder , X_test , device , embed_dim ) y_train = y_train . to ( device ) y_test = y_test . to ( device ) probe = MLPProbe ( input_dim = embed_dim ) . to ( device ) optimizer = optim . AdamW ( probe . parameters (), lr = 1e-3 , weight_decay = 1e-4 ) loss_fn = nn . MSELoss () print ( \" \\n Training probe...\" ) best_test_loss = float ( 'inf' ) for epoch in range ( 1000 ): probe . train () preds = probe ( feat_train ) loss = loss_fn ( preds , y_train ) optimizer . zero_grad () loss . backward () optimizer . step () if epoch % 100 == 0 : probe . eval () with torch . no_grad (): test_preds = probe ( feat_test ) test_loss = loss_fn ( test_preds , y_test ) test_preds_px = test_preds * 64 y_test_px = y_test * 64 pixel_error = torch . mean ( torch . sqrt ( torch . sum (( test_preds_px - y_test_px ) ** 2 , dim = 1 ))) if test_loss < best_test_loss : best_test_loss = test_loss print ( f \"Epoch { epoch } | Test Loss: { test_loss . item () : .6f } | Pixel Error: { pixel_error . item () : .2f } px\" ) print ( f \" \\n Final Test Loss: { best_test_loss : .6f } \" ) return probe , X_test , y_test def visualize_predictions ( encoder , probe , X_test , y_test , device , embed_dim = 384 , n_examples = 3 ): \"\"\" Visualize model predictions vs ground truth \"\"\" encoder . eval () probe . eval () fig , axes = plt . subplots ( 1 , n_examples , figsize = ( 5 * n_examples , 5 )) if n_examples == 1 : axes = [ axes ] for i , ax in enumerate ( axes ): idx = np . random . randint ( len ( X_test )) video = X_test [ idx : idx + 1 ] . to ( device ) true_pos_norm = y_test [ idx ] . cpu () . numpy () true_pos = true_pos_norm * 64 with torch . no_grad (): feat = extract_features ( encoder , video , device , embed_dim , batch_size = 1 ) pred_pos_norm = probe ( feat ) . cpu () . numpy ()[ 0 ] pred_pos = pred_pos_norm * 64 error = np . sqrt ( np . sum (( true_pos - pred_pos ) ** 2 )) last_frame = video [ 0 , 0 , - 1 ] . cpu () . numpy () ax . imshow ( last_frame , cmap = 'gray' , vmin =- 1 , vmax = 1 ) ax . scatter ( true_pos [ 1 ], true_pos [ 0 ], c = 'lime' , s = 200 , label = 'Ground Truth' , marker = 'o' , edgecolors = 'black' , linewidths = 2 ) ax . scatter ( pred_pos [ 1 ], pred_pos [ 0 ], c = 'red' , s = 200 , label = 'Prediction' , marker = 'x' , linewidths = 3 ) ax . plot ([ true_pos [ 1 ], pred_pos [ 1 ]], [ true_pos [ 0 ], pred_pos [ 0 ]], 'r--' , alpha = 0.6 , linewidth = 2 ) ax . set_title ( f \"Error: { error : .2f } px\" ) ax . legend () ax . axis ( 'off' ) plt . tight_layout () plt . show ()","title":"The Code Implementation"},{"location":"#phase-4-the-action-conditioned-world-model","text":"Now that our AI understands the laws of physics (Phase 1 & 2), we give it a body. We transition from a passive observer to an active agent .","title":"Phase 4: The Action-Conditioned World Model"},{"location":"#1-the-motor-cortex-action-model","text":"We attach a secondary neural network to the frozen Encoder. This is the Action Model . The Function Input: The current \"thought\" (Latent State \\(z_t\\) ) + An Action (Velocity Vector \\(a_t\\) ). Output: The predicted next \"thought\" ( \\(z_{t+1}\\) ). The Goal: It learns the transition function of the universe: $ \\(f(z_t, a_t) \\rightarrow z_{t+1}\\) $","title":"1. The \"Motor Cortex\" (Action Model)"},{"location":"#the-architecture-cross-attention-for-control","text":"How do we fuse a \"Thought\" (complex video tokens) with an \"Action\" (simple velocity vector)? We use Cross-Attention . The \"Thought\" is the Query, and the \"Action\" acts as the Key/Value that modulates the thought. motor_cortex.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 import torch import torch.nn as nn class ActionEncoder ( nn . Module ): \"\"\" Transforms raw actions (e.g., velocity [vx, vy]) into a high-dimensional vector. \"\"\" def __init__ ( self , action_dim = 2 , embed_dim = 384 ): super () . __init__ () self . net = nn . Sequential ( nn . Linear ( action_dim , 128 ), nn . LayerNorm ( 128 ), nn . ReLU (), nn . Linear ( 128 , 256 ), nn . LayerNorm ( 256 ), nn . ReLU (), nn . Linear ( 256 , embed_dim ) ) def forward ( self , actions ): return self . net ( actions ) class ActionConditionedPredictor ( nn . Module ): \"\"\" The Brain's Simulator: Takes current state tokens and an action, then hallucinates the next state tokens. \"\"\" def __init__ ( self , embed_dim = 384 , num_layers = 4 ): super () . __init__ () self . embed_dim = embed_dim self . action_encoder = ActionEncoder ( action_dim = 2 , embed_dim = embed_dim ) self . cross_attn = nn . MultiheadAttention ( embed_dim , num_heads = 8 , batch_first = True , dropout = 0.1 ) layer = nn . TransformerEncoderLayer ( d_model = embed_dim , nhead = 8 , dim_feedforward = 1024 , batch_first = True , norm_first = True , dropout = 0.1 ) self . dynamics = nn . TransformerEncoder ( layer , num_layers = num_layers ) self . norm = nn . LayerNorm ( embed_dim ) self . pred_head = nn . Linear ( embed_dim , embed_dim ) def forward ( self , state_tokens , actions ): # Embed Action: [Batch, 2] -> [Batch, 1, Embed_Dim] action_embed = self . action_encoder ( actions ) . unsqueeze ( 1 ) state_conditioned , _ = self . cross_attn ( state_tokens , action_embed , action_embed ) state_conditioned = state_tokens + state_conditioned # Predict Next State next_state = self . dynamics ( state_conditioned ) next_state = self . norm ( next_state ) return self . pred_head ( next_state )","title":"The Architecture: Cross-Attention for Control"},{"location":"#the-interpreter-position-decoder","text":"The Motor Cortex works entirely in \"dream space\" (embeddings). To plan a path, we need to translate these dreams back into coordinates \\((X, Y)\\) so we can calculate the distance to our goal. Interpreter.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 class PositionDecoder ( nn . Module ): \"\"\" Reads the 'dream' tokens and extracts the object's predicted position. Used by the planner to check if the dream leads to the goal. \"\"\" def __init__ ( self , embed_dim = 384 ): super () . __init__ () self . net = nn . Sequential ( nn . Linear ( embed_dim , 256 ), nn . LayerNorm ( 256 ), nn . ReLU (), nn . Linear ( 256 , 128 ), nn . LayerNorm ( 128 ), nn . ReLU (), nn . Linear ( 128 , 2 ), nn . Sigmoid () ) def forward ( self , state_tokens ): pooled = state_tokens . max ( dim = 1 ) . values return self . net ( pooled )","title":"The \"Interpreter\" (Position Decoder)"},{"location":"#the-training-loop","text":"We do not need to label data by hand. We can reuse our simulator to self-supervise the agent. Freeze the Eye: The Encoder is now locked. It provides stable representations of the world. Random Actions: We generate videos where we randomly \"push\" the digit around. Prediction: We train the ActionConditionedPredictor to minimize the difference between its predicted latent state and the actual latent state produced by the Encoder for the next frame. Mathematical Objective We minimize a composite loss function \\(\\mathcal{L}_{total}\\) that ensures the agent predicts both the correct abstract concept (Latent State) and the correct physical location (decoded position). \\[ \\mathcal{L}_{total} = \\mathcal{L}_{dynamics} + \\mathcal{L}_{position} \\] 1. Dynamics Loss (Latent Space): \\[\\mathcal{L}_{dynamics} = || z_{t+1}^{pred} - z_{t+1}^{teacher} ||^2\\] 2. Position Loss (Physical Space): To ground the \"dreams\" in reality, we force the decoded position of the predicted state to match the true coordinate \\(p_{t+1}\\) . \\[\\mathcal{L}_{position} = || \\text{Dec}(z_{t+1}^{teacher}) - p_{t+1} ||^2 + 0.5 \\cdot || \\text{Dec}(z_{t+1}^{pred}) - p_{t+1} ||^2\\] trainer.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 def train_action_predictor ( frozen_encoder , batch_size = 32 , epochs = 100 , lr = 1e-4 , device = 'cuda' ): print ( \"=\" * 50 ) print ( \"STAGE 2: TRAINING ACTION PREDICTOR\" ) print ( \"=\" * 50 ) device = torch . device ( device if torch . cuda . is_available () else \"cpu\" ) frozen_encoder . eval () for p in frozen_encoder . parameters (): p . requires_grad = False world = ControllableMicroWorld ( batch_size = batch_size , seq_len = 16 ) predictor = ActionConditionedPredictor () . to ( device ) pos_decoder = PositionDecoder () . to ( device ) optimizer = optim . AdamW ( list ( predictor . parameters ()) + list ( pos_decoder . parameters ()), lr = lr ) loss_fn = nn . MSELoss () for epoch in range ( epochs ): videos , actions , states = world . create_controlled_universe () videos , actions , states = videos . to ( device ), actions . to ( device ), states . to ( device ) with torch . no_grad (): full_embeds = frozen_encoder ( videos ) B , _ , D = full_embeds . shape time_sep_embeds = full_embeds . view ( B , 16 , 256 , D ) total_loss = 0 for t in range ( 15 ): curr_state_z = time_sep_embeds [:, t ] next_state_z_gt = time_sep_embeds [:, t + 1 ] action_t = actions [:, t ] next_state_z_pred = predictor ( curr_state_z , action_t ) true_pos_next = states [:, t + 1 , : 2 ] pred_pos_from_gt = pos_decoder ( next_state_z_gt ) pred_pos_from_pred = pos_decoder ( next_state_z_pred ) loss_dynamics = loss_fn ( next_state_z_pred , next_state_z_gt ) loss_pos = loss_fn ( pred_pos_from_gt , true_pos_next ) + 0.5 * loss_fn ( pred_pos_from_pred , true_pos_next ) loss = loss_dynamics + loss_pos optimizer . zero_grad () loss . backward () torch . nn . utils . clip_grad_norm_ ( predictor . parameters (), 1.0 ) optimizer . step () total_loss += loss . item () if epoch % 10 == 0 : print ( f \"Epoch { epoch } / { epochs } | Avg Loss: { total_loss / 15 : .5f } \" ) return predictor , pos_decoder","title":"The Training Loop"},{"location":"#the-mpc-planner-the-strategist","text":"Once the Motor Cortex is trained, we don't just predict one step; we plan. We use Model Predictive Control (MPC) . The Planning Loop (CEM) We use the Cross-Entropy Method (CEM) , a sampling-based optimization algorithm. Initialize: Create a Gaussian distribution \\(\\mathcal{N}(\\mu, \\sigma)\\) for action sequences. Sample: Draw \\(N=200\\) random action sequences from this distribution. Evaluate: Run these sequences through the predictor to get latent states \\(z_{1:T}\\) . Decode positions and calculate distance to the goal. Select Elites: Pick the top \\(10\\%\\) sequences with the lowest cost. Refit: Update \\(\\mu\\) and \\(\\sigma\\) to match the elites. Repeat: Iterate 5 times to refine the plan. MPC.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 class AdvancedMPCPlanner : def __init__ ( self , encoder , predictor , pos_decoder , device = 'cuda' ): self . encoder = encoder self . predictor = predictor self . pos_decoder = pos_decoder self . device = device def encode_frame ( self , frame ): with torch . no_grad (): return self . encoder ( frame ) def plan_cem ( self , start_frame , goal_frame , horizon = 16 , num_samples = 200 , iterations = 5 ): print ( f \"Planning horizon { horizon } ...\" ) self . encoder . eval () self . predictor . eval () self . pos_decoder . eval () start_z = self . encode_frame ( start_frame ) goal_z = self . encode_frame ( goal_frame ) with torch . no_grad (): goal_pos = self . pos_decoder ( goal_z ) mean = torch . zeros ( horizon , 2 , device = self . device ) std = torch . ones ( horizon , 2 , device = self . device ) * 0.5 best_action_seq = None best_cost = float ( 'inf' ) for i in range ( iterations ): noise = torch . randn ( num_samples , horizon , 2 , device = self . device ) actions_batch = mean . unsqueeze ( 0 ) + noise * std . unsqueeze ( 0 ) curr_z = start_z . repeat ( num_samples , 1 , 1 ) cumulative_cost = torch . zeros ( num_samples , device = self . device ) with torch . no_grad (): for t in range ( horizon ): act = actions_batch [:, t ] next_z = self . predictor ( curr_z , act ) pred_pos = self . pos_decoder ( next_z ) dist = torch . norm ( pred_pos - goal_pos , dim = 1 ) if t == horizon - 1 : cumulative_cost += dist * 10.0 else : cumulative_cost += dist curr_z = next_z elite_k = int ( num_samples * 0.1 ) elite_idxs = torch . argsort ( cumulative_cost )[: elite_k ] elites = actions_batch [ elite_idxs ] mean = elites . mean ( dim = 0 ) std = elites . std ( dim = 0 ) + 0.1 current_best_cost = cumulative_cost [ elite_idxs [ 0 ]] . item () if current_best_cost < best_cost : best_cost = current_best_cost best_action_seq = elites [ 0 ] print ( f \" CEM Iter { i } : Best Cost { current_best_cost : .4f } \" ) return best_action_seq , best_cost","title":"The MPC Planner (The Strategist)"},{"location":"#execution-the-demo-suite","text":"To verify the system, we implement a simulation harness (simulate_rollout) that manually executes physics using the exact logic from the MicroWorld, and a demo runner that performs open-loop and closed-loop control. demo.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 def simulate_rollout ( world_factory , start_state_tuple , action_sequence , device ): \"\"\" Manually simulates physics using the exact logic from MicroWorldFactory. Returns: [Batch, Channel, Time, Height, Width] -> [1, 1, T, 64, 64] \"\"\" ( d1 , p1 , v1 , d2 , p2 , v2 ) = copy . deepcopy ( start_state_tuple ) actions = action_sequence . cpu () horizon = len ( actions ) frames = [] for t in range ( horizon ): # Render frame = torch . zeros ( 1 , 64 , 64 ) r1 , c1 = p1 . round () . int () . tolist () r1 = max ( 0 , min ( r1 , 64 - 28 )) c1 = max ( 0 , min ( c1 , 64 - 28 )) frame [ 0 , r1 : r1 + 28 , c1 : c1 + 28 ] = torch . max ( frame [ 0 , r1 : r1 + 28 , c1 : c1 + 28 ], d1 ) r2 , c2 = p2 . round () . int () . tolist () r2 = max ( 0 , min ( r2 , 64 - 28 )) c2 = max ( 0 , min ( c2 , 64 - 28 )) frame [ 0 , r2 : r2 + 28 , c2 : c2 + 28 ] = torch . max ( frame [ 0 , r2 : r2 + 28 , c2 : c2 + 28 ], d2 ) frames . append (( frame - 0.5 ) * 2.0 ) # Update Physics act = actions [ t ] v1 += act v1 = torch . clamp ( v1 , - 5.0 , 5.0 ) p1 += v1 p2 += v2 # Bounce logic if p1 [ 0 ] < 0 or p1 [ 0 ] > ( 64 - 28 ): v1 [ 0 ] *= - 1 ; p1 [ 0 ] = torch . clamp ( p1 [ 0 ], 0 , 64 - 28 ) if p1 [ 1 ] < 0 or p1 [ 1 ] > ( 64 - 28 ): v1 [ 1 ] *= - 1 ; p1 [ 1 ] = torch . clamp ( p1 [ 1 ], 0 , 64 - 28 ) if p2 [ 0 ] < 0 or p2 [ 0 ] > ( 64 - 28 ): v2 [ 0 ] *= - 1 ; p2 [ 0 ] = torch . clamp ( p2 [ 0 ], 0 , 64 - 28 ) if p2 [ 1 ] < 0 or p2 [ 1 ] > ( 64 - 28 ): v2 [ 1 ] *= - 1 ; p2 [ 1 ] = torch . clamp ( p2 [ 1 ], 0 , 64 - 28 ) # FIXED: Return [1, 1, T, 64, 64] return torch . stack ( frames ) . permute ( 1 , 0 , 2 , 3 ) . unsqueeze ( 0 ) def create_rollout_video ( start_frame , goal_frame , planned_frames , filename = \"rollout.mp4\" ): \"\"\" Generates a side-by-side video: Goal | Planned Execution planned_frames shape: [1, 1, T, 64, 64] \"\"\" frames = planned_frames [ 0 , 0 ] . cpu () . numpy () # [T, 64, 64] goal_img = goal_frame [ 0 , 0 , 0 ] . cpu () . numpy () fig , ( ax1 , ax2 ) = plt . subplots ( 1 , 2 , figsize = ( 8 , 4 )) im1 = ax1 . imshow ( frames [ 0 ], cmap = 'gray' , vmin =- 1 , vmax = 1 ) ax1 . set_title ( \"Planned Execution\" ) ax1 . axis ( 'off' ) ax2 . imshow ( goal_img , cmap = 'gray' , vmin =- 1 , vmax = 1 ) ax2 . set_title ( \"Target Goal\" ) ax2 . axis ( 'off' ) def update ( frame_idx ): im1 . set_array ( frames [ frame_idx ]) return [ im1 ] ani = animation . FuncAnimation ( fig , update , frames = len ( frames ), blit = True ) try : ani . save ( filename , writer = 'ffmpeg' , fps = 5 ) print ( f \"Video saved to { filename } \" ) except : print ( \"FFmpeg not found. Skipping video save.\" ) plt . close () def run_advanced_demo (): device = 'cuda' if torch . cuda . is_available () else 'cpu' # 1. Train Models enc_student , enc_teacher = train_world_model ( epochs = 30 , device = device ) predictor , pos_decoder = train_action_predictor ( enc_teacher , epochs = 30 , device = device ) planner = AdvancedMPCPlanner ( enc_teacher , predictor , pos_decoder , device ) # --------------------------------------------------------- # DEMO 1: Visualize Rollout # --------------------------------------------------------- print ( \" \\n \" + \"=\" * 50 ) print ( \"DEMO 1: VISUALIZING ROLLOUT\" ) print ( \"=\" * 50 ) world = ControllableMicroWorld ( batch_size = 1 , seq_len = 16 ) idx1 = np . random . randint ( world . num_atoms ) idx2 = np . random . randint ( world . num_atoms ) digit1 , digit2 = world . atoms [ idx1 ], world . atoms [ idx2 ] pos1 = torch . rand ( 2 ) * ( 64 - 28 ) pos2 = torch . rand ( 2 ) * ( 64 - 28 ) vel1 = torch . randn ( 2 ) * 2.0 vel2 = torch . randn ( 2 ) * 2.0 start_tuple = ( digit1 , pos1 . clone (), vel1 . clone (), digit2 , pos2 . clone (), vel2 . clone ()) # FIXED: Create start frame manually. Shape [1, 1, 1, 64, 64] start_frame_sim = simulate_rollout ( None , start_tuple , torch . zeros ( 1 , 2 ), device ) . to ( device ) # Generate a dummy goal rand_actions = torch . randn ( 16 , 2 ) goal_seq = simulate_rollout ( None , start_tuple , rand_actions , device ) # FIXED: Slice to get last frame while keeping dims [1, 1, 1, 64, 64] goal_frame = goal_seq [:, :, - 1 :] . to ( device ) # Plan best_actions , _ = planner . plan_cem ( start_frame_sim , goal_frame , horizon = 16 ) # Execute Plan predicted_rollout = simulate_rollout ( None , start_tuple , best_actions , device ) # Create Video create_rollout_video ( start_frame_sim , goal_frame , predicted_rollout , \"demo1_rollout.mp4\" ) # --------------------------------------------------------- # DEMO 2: Harder Goals (30 steps) # --------------------------------------------------------- print ( \" \\n \" + \"=\" * 50 ) print ( \"DEMO 2: LONG HORIZON PLANNING (30 STEPS)\" ) print ( \"=\" * 50 ) long_actions = torch . randn ( 30 , 2 ) long_goal_seq = simulate_rollout ( None , start_tuple , long_actions , device ) long_goal_frame = long_goal_seq [:, :, - 1 :] . to ( device ) best_actions_long , _ = planner . plan_cem ( start_frame_sim , long_goal_frame , horizon = 30 , iterations = 10 ) long_rollout = simulate_rollout ( None , start_tuple , best_actions_long , device ) create_rollout_video ( start_frame_sim , long_goal_frame , long_rollout , \"demo2_long_horizon.mp4\" ) # --------------------------------------------------------- # DEMO 3: Closed-Loop Control # --------------------------------------------------------- print ( \" \\n \" + \"=\" * 50 ) print ( \"DEMO 3: CLOSED-LOOP CONTROL (Receding Horizon)\" ) print ( \"=\" * 50 ) current_tuple = copy . deepcopy ( start_tuple ) current_frame = start_frame_sim executed_frames = [] for step in range ( 10 ): print ( f \"Closed Loop Step { step + 1 } /10\" ) # Plan for horizon 10 actions_plan , _ = planner . plan_cem ( current_frame , goal_frame , horizon = 10 , iterations = 3 , num_samples = 50 ) first_action = actions_plan [ 0 : 1 ] # Physics Step next_seq = simulate_rollout ( None , current_tuple , first_action , device ) next_frame = next_seq . to ( device ) # Shape [1, 1, 1, 64, 64] executed_frames . append ( next_frame ) # Update State Tuple d1 , p1 , v1 , d2 , p2 , v2 = current_tuple v1 += first_action [ 0 ] . cpu () v1 = torch . clamp ( v1 , - 5.0 , 5.0 ) p1 += v1 p2 += v2 current_tuple = ( d1 , p1 , v1 , d2 , p2 , v2 ) current_frame = next_frame print ( \"Closed-loop execution complete.\" ) if __name__ == \"__main__\" : run_advanced_demo ()","title":"Execution (The Demo Suite)"},{"location":"#references","text":"[ABB+23] Mahmoud Assran, Johann Ball\u00e9, Charles Blondel, J\u00f6rg Bornschein, Mathilde Caron, Rianne M\u00fcller, Mahmoud Assran, Sylvain Gelly, and Gabriel Synnaeve. Self-supervised learning from images with a joint-embedding predictive architecture. CVPR, 2023. PDF Link (Accessed: Jan 5, 2026). [App25] Apple Machine Learning Research. Rethinking JEPA: Compute-efficient video SSL with frozen teachers. 2025. Article Link (Accessed: Jan 5, 2026). [K\u013125] \u0130lyurek K\u0131l\u0131\u00e7. Beyond next-token prediction: Yann LeCun\u2019s JEPA and the quest for AI common sense \u2014 where everything is an abstraction. Medium, 2025. Article Link (Accessed: Jan 5, 2026). [LeC] Yann LeCun. Energy-based learning. PDF Link (Accessed: Jan 5, 2026). [LeC22] Yann LeCun. A path towards autonomous machine intelligence version 0.9.2. OpenReview, 62(1):1\u201362, 2022. [Let24] Malcolm Lett. Critical review of LeCun\u2019s introductory JEPA paper. Medium, 2024. Article Link (Accessed: Jan 5, 2026). [Mina] Emergent Mind. Joint-embedding predictive architectures. Topic Link (Accessed: Jan 5, 2026). [Minb] Emergent Mind. VICReg-based JEPA overview. Topic Link (Accessed: Jan 5, 2026). [Red25a] Reddit r/MachineLearning. Why does BYOL/JEPA like models work? How does EMA prevent model collapse? 2025. Thread Link (Accessed: Jan 5, 2026). [Red25b] Reddit r/singularity. Could someone explain what each of these architectures are that LeCun claims could lead to AGI? 2025. Thread Link (Accessed: Jan 5, 2026). [Sha22] Shaped. Yann LeCun: A path towards autonomous machine intelligence. 2022. Blog Link (Accessed: Jan 5, 2026). [Und25] Cogni Down Under. A new kind of AI is emerging and it\u2019s better than LLMs? Medium, Dec 2025. Article Link (Accessed: Jan 5, 2026).","title":"References"},{"location":"#appendix-mathematical-formalism-of-v-jepa-components","text":"This appendix details the mathematical operations underpinning the Video Joint Embedding Predictive Architecture (V-JEPA). We decompose the model into four primary components: The Eye (Encoder), The GPS (Positional Embedding), The Game (Masking Strategy), and The Brain (Predictor).","title":"Appendix: Mathematical Formalism of V-JEPA Components"},{"location":"#the-eye-mathematics-of-3d-convolution","text":"The \"Eye\" functions mathematically as a linear projection from the high-dimensional pixel space to a lower-dimensional feature space. We employ a 3D Convolutional Neural Network (CNN) operation to achieve this transformation.","title":"The Eye: Mathematics of 3D Convolution"},{"location":"#the-input-tensor-v","text":"The input video is represented as a 4D tensor (omitting the batch dimension for clarity): \\[ V \\in \\mathbb{R}^{T \\times H \\times W} \\] Where: \\(T=16\\) (Time frames) \\(H=64, W=64\\) (Height and Width in pixels) The value range is normalized: \\(V_{t,y,x} \\in [-1, 1]\\)","title":"The Input Tensor (\\(V\\))"},{"location":"#the-transformation-the-kernel-k","text":"The Conv3d layer learns a set of filters (weights). Assuming an embedding dimension of \\(D=256\\) , we have 256 such filters. Each filter \\(k\\) is a spatiotemporal cube of weights: \\[ W_k \\in \\mathbb{R}^{2 \\times 8 \\times 8} \\] This corresponds to a kernel size covering 2 frames in time and an \\(8 \\times 8\\) pixel patch in space.","title":"The Transformation (The Kernel \\(K\\))"},{"location":"#the-convolution-operation","text":"To derive the scalar embedding value for a specific tubelet at grid position \\((t, i, j)\\) and filter \\(k\\) , we compute the dot product between the video pixels and the kernel weights, adding a bias term \\(b_k\\) . This operation is formally a weighted sum: \\[ E_{t,i,j}^{(k)} = \\left( \\sum_{\\tau=0}^{1} \\sum_{y=0}^{7} \\sum_{x=0}^{7} V_{t+\\tau, i+y, j+x} \\cdot W_k(\\tau, y, x) \\right) + b_k \\]","title":"The Convolution Operation"},{"location":"#the-output-flattening","text":"Applying this operation across the entire video volume yields a grid of embedding vectors. We flatten this 3D grid into a sequence of vectors, referred to as \"tokens\": \\[ Z = \\{ z_1, z_2, \\dots, z_{512} \\} \\] Where each token \\(z_n \\in \\mathbb{R}^{256}\\) . Summary: The Eye transforms a spatiotemporal block of 128 pixels ( \\(2 \\times 8 \\times 8\\) ) into a single vector of 256 scalars, effectively compressing local visual information (e.g., \"contains a curved line moving right\") into a latent representation.","title":"The Output (Flattening)"},{"location":"#the-gps-mathematics-of-factorized-embeddings","text":"Transformers process inputs as sets, not sequences; a set \\(\\{A, B, C\\}\\) is mathematically identical to \\(\\{C, A, B\\}\\) . Without explicit positional information (GPS), the model lacks the inductive bias to understand that Frame 1 temporally precedes Frame 2.","title":"The GPS: Mathematics of Factorized Embeddings"},{"location":"#the-problem","text":"Standard 1D positional embeddings assign a unique vector \\(P_n\\) to the \\(n\\) -th token in a flattened sequence: \\[ z_n' = z_n + P_n \\] However, in a video volume, token \\(n=10\\) might be spatially adjacent to token \\(n=18\\) (in the row below) or temporally adjacent to token \\(n=74\\) (in the next frame). A single scalar index \\(n\\) destroys these 3D relational structures.","title":"The Problem"},{"location":"#the-solution-factorized-addition","text":"To preserve 3D structure, we define three distinct vector spaces for positional embeddings: Time Space: \\(E_T \\in \\mathbb{R}^{8 \\times D}\\) Height Space: \\(E_H \\in \\mathbb{R}^{8 \\times D}\\) Width Space: \\(E_W \\in \\mathbb{R}^{8 \\times D}\\) For a token located at grid coordinates \\((t, h, w)\\) , the positional embedding vector \\(P_{(t,h,w)}\\) is calculated as the element-wise sum of the three component vectors: \\[ P_{(t,h,w)} = E_T[t] + E_H[h] + E_W[w] \\]","title":"The Solution (Factorized Addition)"},{"location":"#why-summation","text":"One might ask: Why not concatenation? Concatenation would triple the dimension of the embedding vector ( \\(3 \\times D\\) ). By using summation, we maintain the vector dimension \\(D\\) . In high-dimensional geometry, the sum of different random vectors results in a new vector that is nearly orthogonal (unique) to the original components. The neural network learns to \"disentangle\" this sum to recover the underlying spatiotemporal coordinates.","title":"Why Summation?"},{"location":"#the-game-mathematics-of-set-partitioning","text":"The masking strategy employed in JEPAs is fundamentally a set theory operation designed to create a self-supervised learning signal.","title":"The Game: Mathematics of Set Partitioning"},{"location":"#the-universe-of-indices-u","text":"We define \\(U\\) as the set of indices for all tokens in the video: \\[ U = \\{ 1, 2, \\dots, 512 \\} \\]","title":"The Universe of Indices (\\(U\\))"},{"location":"#the-partition","text":"We randomly partition \\(U\\) into two disjoint subsets: Context Set ( \\(I_{context}\\) ): The indices of visible tokens. Target Set ( \\(I_{target}\\) ): The indices of hidden tokens. This partition satisfies two conditions: \\[ I_{context} \\cap I_{target} = \\emptyset \\] \\[ I_{context} \\cup I_{target} = U \\]","title":"The Partition"},{"location":"#the-masking-ratio-rho","text":"The scalar \\(\\rho\\) defines the difficulty of the task: \\[ |I_{target}| = \\rho \\cdot |U| \\] For example, if \\(\\rho = 0.6\\) , we obscure 60% of the tokens.","title":"The Masking Ratio (\\(\\rho\\))"},{"location":"#the-input-construction","text":"We construct two distinct inputs for the neural networks based on this partition: Encoder Input ( \\(X_{enc}\\) ): Contains only the subset of vectors \\(Z\\) belonging to the context. \\[ X_{enc} = \\{ z_i + P_i \\mid i \\in I_{context} \\} \\] Predictor Input ( \\(X_{pred}\\) ): A hybrid sequence containing real data from the context and learned placeholders for the targets. \\[ X_{pred} = X_{enc} \\cup \\{ M + P_j \\mid j \\in I_{target} \\} \\] Where \\(M\\) is a learnable \"Mask Token\" vector. Crucial Math: The predictor receives the GPS coordinates ( \\(P_j\\) ) of the missing parts but not their content ( \\(z_j\\) ). This creates the query: \"Given the surrounding context, what content belongs at coordinate \\(P_j\\) ?\"","title":"The Input Construction"},{"location":"#the-brain-mathematics-of-self-attention","text":"The Predictor is a Transformer architecture. Its core mathematical operation is Scaled Dot-Product Attention, which allows the model to route information between tokens.","title":"The Brain: Mathematics of Self-Attention"},{"location":"#the-input-matrix-x","text":"The input to a transformer block is a matrix of size \\(N \\times D\\) , where \\(N\\) is the number of tokens and \\(D\\) is the embedding dimension.","title":"The Input Matrix (\\(X\\))"},{"location":"#projections","text":"We project the input \\(X\\) into three distinct views using learnable weight matrices \\(W_Q, W_K, W_V\\) : Queries ( \\(Q = X W_Q\\) ): Represents what the token is looking for. Keys ( \\(K = X W_K\\) ): Represents what the token contains. Values ( \\(V = X W_V\\) ): Represents the information the token will pass on.","title":"Projections"},{"location":"#the-attention-equation","text":"The attention mechanism computes a weighted sum of values based on the similarity between queries and keys: \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left( \\frac{QK^T}{\\sqrt{D}} \\right) V \\] Interpretation: \\(QK^T\\) : Calculates a similarity score (dot product) between every pair of tokens. For example, a \"Mask Token at (Time 2, Pos A)\" might generate a high dot product with a \"Context Token at (Time 1, Pos A)\" due to temporal proximity. Softmax: Normalizes these scores into probabilities that sum to 1. This forms the \"attention map.\" \\(\\times V\\) : Computes the weighted average of the Values. If a Mask Token attends 90% to a specific Context Token, it aggregates 90% of that context's information vector.","title":"The Attention Equation"},{"location":"#the-optimization-goal-the-loss-function","text":"The objective is to minimize the distance between the Predictor's output for the masked regions ( \\(\\hat{z}\\) ) and the Teacher's output ( \\(y\\) ). We typically utilize the L2 distance (Mean Squared Error): \\[ \\mathcal{L} = \\sum_{j \\in I_{target}} || \\hat{z}_j - y_j ||^2 \\] Where: \\(\\hat{z}_j\\) : The latent vector predicted by the student network (the brain). \\(y_j\\) : The latent vector extracted by the teacher network (the eye) from the full, unmasked video. Gradient Descent: We calculate the gradient of the Loss \\(\\mathcal{L}\\) with respect to the weights ( \\(\\nabla W\\) ) and update the weights to minimize this error.","title":"The Optimization Goal (The Loss Function)"},{"location":"#summary-of-the-flow","text":"Eye: \\(Pixels \\xrightarrow{\\text{Sum \\& Dot Product}} Embeddings\\) GPS: \\(Embeddings \\xrightarrow{\\text{Vector Addition}} Located\\_Embeddings\\) Game: \\(Located\\_Embeddings \\xrightarrow{\\text{Set Split}} Context \\cup Targets\\) Brain: \\(Context \\xrightarrow{\\text{Matrix Multiplication (Attention)}} Predictions\\)","title":"Summary of the Flow"}]}